{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2174f73d",
   "metadata": {},
   "source": [
    "# Spark Environment Options and Dependencies\n",
    "\n",
    "* This notebook includes examples of how to connect and configure Spark to run in a local environment and to manage the set of dependencies and options required to read/write data to an S3 compatible object storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38bd27e5-2c1b-48b3-a7e4-5d69da267aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting delta\n",
      "  Downloading delta-0.4.2.tar.gz (4.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: delta\n",
      "  Building wheel for delta (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for delta: filename=delta-0.4.2-py3-none-any.whl size=2928 sha256=01782673ecb5f9201ee04c7f44764f4552a59bd327cbdf4fadd308ca1d0dadb3\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/06/c9/f4/15ff81c648b9fc73aae5886b41204ada25bd73cbb41b9fad78\n",
      "Successfully built delta\n",
      "Installing collected packages: delta\n",
      "Successfully installed delta-0.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c173f02b-719b-45bc-b567-ce55f2d55bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting delta-spark\n",
      "  Downloading delta_spark-2.0.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from delta-spark) (4.10.1)\n",
      "Requirement already satisfied: pyspark<3.3.0,>=3.2.0 in /usr/local/spark-3.2.0-bin-hadoop3.2/python (from delta-spark) (3.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=1.0.0->delta-spark) (3.7.0)\n",
      "Collecting py4j==0.10.9.2\n",
      "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.8/198.8 KB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: py4j, delta-spark\n",
      "Successfully installed delta-spark-2.0.0 py4j-0.10.9.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install delta-spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cffeaaf-4701-4114-94ee-fd9e6b61f555",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spark Session\n",
    "\n",
    "* This is the entry point to Spark SQL. It is one of the very first objects you create while developing a Spark SQL application.\n",
    "* As a Spark developer, you create a SparkSession using the SparkSession.builder method (that gives you access to Builder API that you use to configure the session)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b08107e4-ca56-4f5f-869d-bbe29a58c1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "import os\n",
    "from delta.tables import *\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType,array,ArrayType,DateType,TimestampType, FloatType\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import udf\n",
    "import hashlib\n",
    "import datetime\n",
    "import urllib.request\n",
    "import json\n",
    "from datetime import timedelta, date\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SQLContext\n",
    "from itertools import islice\n",
    "from pyspark.sql.functions import col\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3b4f70-be0b-48c9-bb00-9c61f859f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builder API\n",
    "# Spark session & context\n",
    "spark=SparkSession.builder.master(\"local\").appName(\"Hive-Test\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "\n",
    "# Get configurations\n",
    "# configurations = spark.sparkContext.getConf().getAll()\n",
    "# for item in configurations: print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c595e45f-c31b-4280-9a74-61e9a8a2a899",
   "metadata": {},
   "source": [
    "## MinIO Storage Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a844d50-ac15-4378-8282-8b24ee9f89bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------------+-----------+----------+--------------+----------------+--------------------+----------------+--------------------+\n",
      "|customer_id| customer_name|             address|       city|postalcode|       country|           phone|               email|     credit_card|          updated_at|\n",
      "+-----------+--------------+--------------------+-----------+----------+--------------+----------------+--------------------+----------------+--------------------+\n",
      "|          1|    Ariel Hale|Ap #660-3260 Pell...|    College|     98362| United States|  1-973-833-9836|amet.metus@Nullat...|5124442517412973|2022-07-21 09:14:...|\n",
      "|          2| Aubrey Norris|Ap #943-1347 Impe...| Coldstream|   D10 5JV|United Kingdom|    07672 321093|sollicitudin@enim...|5103696625359419|2022-07-21 09:14:...|\n",
      "|          3|  Bruno Hebert|    8566 Nisi Avenue| Llangollen|   CE2 4WW|United Kingdom|    02794 010514|Donec.non@dapibus...|5132188470727440|2022-07-21 09:14:...|\n",
      "|          4|     Ira Lucas|936-3011 Convalli...| Shreveport|     67365| United States|  1-117-676-2784|      nec@lectus.net|5164946381862809|2022-07-21 09:14:...|\n",
      "|          5|Hannah Ferrell|P.O. Box 755, 794...|Watson Lake|   C6Y 7M3|        Canada|1 (867) 533-2852|nec@orciluctuset....|5256394502723692|2022-07-21 09:14:...|\n",
      "+-----------+--------------+--------------------+-----------+----------+--------------+----------------+--------------------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read CustomersData From Minio\n",
    "customers = spark.read.option(\"header\",True).csv(\"s3a://bronze/sales/customers/2022/07/02/09/customers.csv\")\n",
    "\n",
    "# Show Top 5 \n",
    "customers.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6442de",
   "metadata": {},
   "source": [
    "## Hive Metastore Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aad890b-76e7-4886-865e-5b0306919a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80188f5b-af4e-4c64-92e9-72c0d42ee045",
   "metadata": {},
   "source": [
    "## Read Data from S3 Object Storage (Minio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2563c9c-0de3-4659-b637-ac9ca13e41ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read SQL\n",
    "customers.createOrReplaceTempView(\"customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa6efa5",
   "metadata": {},
   "source": [
    "## Perform Transformations on Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fc0f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Performing Transformations \n",
    "##########################################\n",
    "\n",
    "uk_customers = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM customers\n",
    "    WHERE country = 'United Kingdom'\n",
    "    ORDER BY customer_id ASC\n",
    "    LIMIT 100\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c1570e",
   "metadata": {},
   "source": [
    "## Writing Results to S3 Object Storage (Minio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488880a9",
   "metadata": {},
   "source": [
    "### Writing in CSV Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26397a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Writing Results to S3\n",
    "##########################################\n",
    "uk_customers.write.option(\"header\",\"true\").csv(\"s3a://silver/CSV/jupyter/United-Kingdom-Customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fddc49",
   "metadata": {},
   "source": [
    "### Writing in Delta Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6075e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Writing Results to S3\n",
    "##########################################\n",
    "uk_customers.write.format(\"delta\").mode(\"overwrite\").option('overwriteSchema','true').save(\"s3a://silver/Delta/customers\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

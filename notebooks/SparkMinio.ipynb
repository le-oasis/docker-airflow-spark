{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Environment Options and Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "This notebook includes examples of how to connect and configure Spark to run in a local environment and to manage the set of dependencies and options required to read/write data to an S3 compatible object storage.\n",
    "\n",
    "### Environment Notes\n",
    "* By convention, credentials and secrets are injected into the runtime as environment variables.\n",
    "  - BASH environment variables are usually named in all caps.\n",
    "  - Python variables (generally) follow [pep8](https://www.python.org/dev/peps/pep-0008/)\n",
    "* AWS/S3 variables are prefixed using `OBJECTS_*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4251094180.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [1]\u001b[0;36m\u001b[0m\n\u001b[0;31m    lazy val spark = SparkSession.builder().appName(\"MinIOTest\").master(\"local[*]\").getOrCreate()\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "System.setProperty(SDKGlobalConfiguration.DISABLE_CERT_CHECKING_SYSTEM_PROPERTY, \"true\")\n",
    "\n",
    "lazy val spark = SparkSession.builder().appName(\"MinIOTest\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "val s3accessKeyAws = \"minio\"\n",
    "val s3secretKeyAws = \"miniosecret\"\n",
    "val connectionTimeOut = \"600000\"\n",
    "val s3endPointLoc: String = \"http://127.0.0.1:9000\"\n",
    "\n",
    "val sourceBucket: String = \"playground\"\n",
    "\n",
    "spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.endpoint\", s3endPointLoc)\n",
    "spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.access.key\", s3accessKeyAws)\n",
    "spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.secret.key\", s3secretKeyAws)\n",
    "spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.connection.timeout\", connectionTimeOut)\n",
    "\n",
    "spark.sparkContext.hadoopConfiguration.set(\"spark.sql.debug.maxToStringFields\", \"100\")\n",
    "spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.connection.ssl.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    " \n",
    "spark.sparkContext._jsc\\\n",
    "     .hadoopConfiguration().set(\"fs.s3a.access.key\", \"minio\")\n",
    "spark.sparkContext._jsc\\\n",
    "     .hadoopConfiguration().set(\"fs.s3a.secret.key\", \"miniosecret\")\n",
    "spark.sparkContext._jsc\\\n",
    "      .hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://127.0.0.1:9000\")\n",
    "spark.sparkContext._jsc\\\n",
    "      .hadoopConfiguration().set(\"fs.s3a.connection.timeout\", \"600000\")\n",
    "spark.sparkContext._jsc\\\n",
    "      .hadoopConfiguration().set(\"spark.sql.debug.maxToStringFields\", \"100\")\n",
    "spark.sparkContext._jsc\\\n",
    "      .hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "spark.sparkContext._jsc\\\n",
    "      .hadoopConfiguration().set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark.sparkContext._jsc\\\n",
    "      .hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark.sparkContext._jsc\\\n",
    "      .hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    " \n",
    "spark.sparkContext._jsc\\\n",
    "     .hadoopConfiguration().set(\"fs.s3a.access.key\", \"minio\")\n",
    "spark.sparkContext._jsc\\\n",
    "     .hadoopConfiguration().set(\"fs.s3a.secret.key\", \"miniosecret\")\n",
    "spark.sparkContext._jsc\\\n",
    "      .hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "spark.sparkContext._jsc\\\n",
    "      .hadoopConfiguration().set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark.sparkContext._jsc\\\n",
    "      .hadoopConfiguration().set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "spark.sparkContext._jsc\\\n",
    "      .hadoopConfiguration().set(\"fs.s3a.multipart.size\", \"104857600\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o35.csv.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:537)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m ratings \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minferSchema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 4\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3://playground/iris.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m ratings\u001b[38;5;241m.\u001b[39mregisterTempTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mratings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:535\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o35.csv.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:537)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "ratings = spark.read\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .csv(\"s3://playground/iris.csv\")\n",
    "ratings.registerTempTable(\"ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, posixpath\n",
    "\n",
    "# User Options\n",
    "USER_NAMESPACE = os.environ.get('HOSTNAME')\n",
    "\n",
    "# Spark Options\n",
    "SPARK_DRIVER_RAM = '3g'\n",
    "SPARK_DRIVER_MAXRESULTSIZE = '4096m'\n",
    "SPARK_WORKER_RAM = '6g'\n",
    "\n",
    "# Access credentials for Object Storage\n",
    "OBJECT_STORAGE_URL = os.environ.get('OBJECTS_ENDPOINT')\n",
    "OBJECT_STORAGE_ACCESSID = os.environ.get('OBJECTS_ACCESSID')\n",
    "OBJECT_STORAGE_SECRET = os.environ.get('OBJECTS_SECRET')\n",
    "OBJECTS_PLAYGROUND_BUCKET = 'playground'\n",
    "OBJECTS_USER_PREFIX = USER_NAMESPACE\n",
    "OBJECT_CONTAINER = 'minio'\n",
    "STORAGEURL_PLAYGROUND = 'playground'\n",
    "\n",
    "# Spark Runtime Options\n",
    "os.environ['PYSPARK_PYTHON'] = 'python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'python3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark runtime dependencies. These options control the `jar` and package files which are part of the Spark runtime. This list provides support for working with AWS specific products and APIs, in addition to an updated set required to work with Hadoop 3. **The `JAR_OPTIONS` variable should remain unchanged. For additional runtime dependencies, add them to the `PACKAGE_OPTIONS` array.**\n",
    "\n",
    "_If you modify `PACKAGE_OPTIONS`, you will also need to add it to the `PYSPARK_SUBMIT_ARGS` string._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--jars /home/jovyan/work/jars/joda-time-2.9.9.jar,/home/jovyan/work/jars/httpclient-4.5.3.jar,/home/jovyan/work/jars/aws-java-sdk-s3-1.11.534.jar,/home/jovyan/work/jars/aws-java-sdk-kms-1.11.534.jar,/home/jovyan/work/jars/aws-java-sdk-dynamodb-1.11.534.jar,/home/jovyan/work/jars/aws-java-sdk-core-1.11.534.jar,/home/jovyan/work/jars/aws-java-sdk-1.11.534.jar,/home/jovyan/work/jars/hadoop-aws-3.1.2.jar,/home/jovyan/work/jars/slf4j-api-1.7.29.jar,/home/jovyan/work/jars/slf4j-log4j12-1.7.29.jar  pyspark-shell'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manage PySpark Runtime Options\n",
    "PACKAGE_OPTIONS = '--packages %s ' % ','.join((        \n",
    "        # 'org.apache.spark:spark-avro_2.12:2.4.4',\n",
    "    ))\n",
    "\n",
    "JAR_OPTIONS = '--jars %s ' % ','.join((\n",
    "        '/home/jovyan/work/jars/joda-time-2.9.9.jar',\n",
    "        '/home/jovyan/work/jars/httpclient-4.5.3.jar',\n",
    "        '/home/jovyan/work/jars/aws-java-sdk-s3-1.11.534.jar',\n",
    "        '/home/jovyan/work/jars/aws-java-sdk-kms-1.11.534.jar',\n",
    "        '/home/jovyan/work/jars/aws-java-sdk-dynamodb-1.11.534.jar',\n",
    "        '/home/jovyan/work/jars/aws-java-sdk-core-1.11.534.jar',\n",
    "        '/home/jovyan/work/jars/aws-java-sdk-1.11.534.jar',\n",
    "        '/home/jovyan/work/jars/hadoop-aws-3.1.2.jar',\n",
    "        '/home/jovyan/work/jars/slf4j-api-1.7.29.jar',\n",
    "        '/home/jovyan/work/jars/slf4j-log4j12-1.7.29.jar',\n",
    "    ))\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = JAR_OPTIONS + ' pyspark-shell'\n",
    "os.environ.get('PYSPARK_SUBMIT_ARGS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7fc02fc3f1c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "\n",
    "# Memory\n",
    "conf.set('spark.driver.memory', SPARK_DRIVER_RAM)\n",
    "conf.set(\"spark.executor.memory\", SPARK_WORKER_RAM)\n",
    "conf.set('spark.driver.maxResultSize', SPARK_DRIVER_MAXRESULTSIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark Driver Hostname and Port\n",
    "* For programs launched from Jupyter, the Jupyter instance will be the Spark driver.\n",
    "* Executor pods need to be able to communicate with the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7fc02fc3f1c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The DNS alias for the Spark driver. Required by executors to report status.\n",
    "conf.set(\"spark.driver.host\", os.environ.get('HOSTNAME'))\n",
    "\n",
    "# Port which the Spark shell should bind to and to which executors will report progress\n",
    "conf.set(\"spark.driver.port\", \"20020\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object Storage Options\n",
    "\n",
    "* To read/write data from object storage (AWS S3 or MinIO), you need to configure the `S3AFileSystem`.\n",
    "* Object storage has an endpoint, access ID, and secret. These credentials have been injected into the environment.\n",
    "* The `OBJECTS_ENDPOINT` is only used for self-hosted environments. For connecting to Amazon's S3, you do not need to set a specific endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7fc02fc3f1c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure S3 Object Storage as filesystem, pass MinIO credentials\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", OBJECT_STORAGE_URL) \\\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", OBJECT_STORAGE_ACCESSID) \\\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", OBJECT_STORAGE_SECRET) \\\n",
    "    .set(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
    "    .set(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "# Configure Avro options\n",
    "conf.set('avro.mapred.ignore.inputs.without.extension', 'true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the Spark Context and Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spark context, create executors\n",
    "conf.setAppName('iotest')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "# Initialize Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check that Spark Session is Active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate sum: 45.0\n"
     ]
    }
   ],
   "source": [
    "# Create a distributed data set to test to the session\n",
    "t = sc.parallelize(range(10))\n",
    "\n",
    "# Calculate the approximate sum of values in the dataset\n",
    "r = t.sumApprox(3)\n",
    "print('Approximate sum: %s' % r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinIO Storage Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: http://minio:9000\n",
      "Access ID: minio\n",
      "Secret Key: miniosecret\n"
     ]
    }
   ],
   "source": [
    "# Create playground and opioids buckets in Amazon S3\n",
    "import boto3\n",
    "from botocore.client import ClientError as AWSClientError\n",
    "\n",
    "s3 = boto3.client('s3',\n",
    "    endpoint_url=OBJECT_STORAGE_URL,\n",
    "    aws_access_key_id=OBJECT_STORAGE_ACCESSID,\n",
    "    aws_secret_access_key=OBJECT_STORAGE_SECRET,\n",
    ")\n",
    "\n",
    "print('URL: %s\\nAccess ID: %s\\nSecret Key: %s' \n",
    "      % (OBJECT_STORAGE_URL, OBJECT_STORAGE_ACCESSID, OBJECT_STORAGE_SECRET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspect bucket structure for minio\n",
      "Bucket minio not present in object store.\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (AllAccessDisabled) when calling the CreateBucket operation: All access to this resource has been disabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInspect bucket structure for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m b)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: s3\u001b[38;5;241m.\u001b[39mhead_bucket(Bucket\u001b[38;5;241m=\u001b[39mb)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m AWSClientError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:508\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:915\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    914\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 915\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (403) when calling the HeadBucket operation: Forbidden",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m AWSClientError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBucket \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m not present in object store.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m b)\n\u001b[0;32m----> 7\u001b[0m     \u001b[43ms3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_bucket\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBucket \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m created successfully.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m b)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:508\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    506\u001b[0m     )\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:915\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    913\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m parsed_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    914\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 915\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (AllAccessDisabled) when calling the CreateBucket operation: All access to this resource has been disabled."
     ]
    }
   ],
   "source": [
    "for b in (OBJECT_CONTAINER, STORAGEURL_PLAYGROUND):\n",
    "    print('Inspect bucket structure for %s' % b)\n",
    "    \n",
    "    try: s3.head_bucket(Bucket=b)\n",
    "    except AWSClientError as err:\n",
    "        print('Bucket %s not present in object store.' % b)\n",
    "        s3.create_bucket(Bucket=b)\n",
    "        print('Bucket %s created successfully.' % b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Data to MinIO (via Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "not all arguments converted during string formatting",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Object Output URL\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m OBJECTURL_TEST \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms3://playground/iris.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mOBJECTS_PLAYGROUND_BUCKET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOBJECTS_USER_PREFIX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(OBJECTURL_TEST)\n",
      "\u001b[0;31mTypeError\u001b[0m: not all arguments converted during string formatting"
     ]
    }
   ],
   "source": [
    "# Object Output URL\n",
    "OBJECTURL_TEST = 's3://playground/iris.csv' % (OBJECTS_PLAYGROUND_BUCKET, OBJECTS_USER_PREFIX)\n",
    "print(OBJECTURL_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a two column table to MinIO as CSV\n",
    "rdd = sc.parallelize([('Mario', 'Red'), ('Luigi', 'Green'), ('Princess', 'Pink')])\n",
    "rdd.toDF(['name', 'color']).write.csv(OBJECTURL_TEST, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data back from MinIO\n",
    "gnames_df = spark.read.format('csv').option('header', True) \\\n",
    "    .load(OBJECTURL_TEST)\n",
    "gnames_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"parent\">\n",
    "<div id=\"popup\" style=\"display: none\">description text here</div>\n",
    "</div>\n",
    "\n",
    "<script>\n",
    "var e = document.getElementById('parent');\n",
    "e.onmouseover = function() {\n",
    "  document.getElementById('popup').style.display = 'block';\n",
    "}\n",
    "e.onmouseout = function() {\n",
    "  document.getElementById('popup').style.display = 'none';\n",
    "}\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing IO to Object Storage (using Python)\n",
    "Spark is able to read/write data to object storage by specifying the file path details in the URL. General form:\n",
    "\n",
    "* Structure: `s3a://{{ bucket }}/{{ object-key-path }}`. Example: `s3a://oak-tree.tech/hello-test.txt`\n",
    "\n",
    "For Python code, you will need to read and write data utilizing the Python `boto3` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example read/write to S3 object storage using Python. Uses the credentials configured earlier in the notebook.\n",
    "\n",
    "# Configure client with credentials\n",
    "# Utilizes the S3 boto client session. See https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html\n",
    "# for reference and details.\n",
    "import boto3\n",
    "s3 = boto3.client('s3',\n",
    "    endpoint_url=OBJECT_STORAGE_URL,\n",
    "    aws_access_key_id=OBJECT_STORAGE_ACCESSID,\n",
    "    aws_secret_access_key=OBJECT_STORAGE_SECRET,\n",
    ")\n",
    "\n",
    "print(OBJECT_STORAGE_URL)\n",
    "\n",
    "# Use requests (Python http client) to retrieve remote data\n",
    "import requests\n",
    "from io import StringIO, BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Choose EITHER this dataset...\n",
    "#\n",
    "# Small(ish) Example (3MB)\n",
    "url = 'https://data.cdc.gov/api/views/p56q-jrxg/rows.csv?accessType=DOWNLOAD&bom=true&format=true'\n",
    "filekey = '%s/opioids.mortality.2004-2016' % OBJECTS_USER_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Bucket: %s. File key: %s' % (OBJECTS_PLAYGROUND_BUCKET, filekey))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve data and write to Amazon S3 using Python, utilizes BytesIO to provide a file stream\n",
    "# to the upload_obj method\n",
    "r0 = requests.get(url, allow_redirects=True)\n",
    "r1 = s3.upload_fileobj(BytesIO(r0.content), OBJECTS_PLAYGROUND_BUCKET, filekey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the file was read read into object storage. Results of method call will be dict (hashmap)\n",
    "for o in s3.list_objects_v2(Bucket=OBJECTS_PLAYGROUND_BUCKET, Prefix=OBJECTS_USER_PREFIX).get('Contents', []):\n",
    "    \n",
    "    # Individual objects in the container will also be represented as dict (hashmap)\n",
    "    print(o.get('Key'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark output URL (uses the Amazon s3a file system as output)\n",
    "# See https://www.redhat.com/en/blog/anatomy-s3a-filesystem-client for a few details\n",
    "# and benefits of S3a.\n",
    "\n",
    "# Read as text\n",
    "spark_in = 's3a://%s/%s' % (OBJECTS_PLAYGROUND_BUCKET, filekey)\n",
    "print('Retrieve data from %s using Spark' % spark_in)\n",
    "df = spark.read.text(spark_in)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read as structured CSV\n",
    "df = spark.read.format('csv') \\\n",
    "  .option(\"inferSchema\", True) \\\n",
    "  .option(\"header\", True) \\\n",
    "  .option('sep', ',') \\\n",
    "  .load(spark_in)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Stage DOT Flight Data to MinIO\n",
    "Prior to being able to analyze data in Spark, if must be staged to a specific location that is accessible to the workers (executors) which will process it. Often this will be to a data lake or storage cluster (such as S3 object storage or HDFS). \n",
    "\n",
    "The following files will be utilized throughout the remainder of the course. Write a simple program to download the data and stage it within the MinIO cluster in this enviornment so that Spark is able to consume it from the provided bucket/file keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLIGHTS_DATAFILES = {\n",
    "    'web-age/wa2843/airports.json': 'https://oak-tree.tech/documents/103/airports.json',\n",
    "    'web-age/wa2843/dot.flights.2018.json': 'https://oak-tree.tech/documents/105/dot.flights.2018.json',\n",
    "    'web-age/wa2843/dot.flights.2017-0102.json': 'https://oak-tree.tech/documents/104/dot.flights.2017-0102.json',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Stage Data to MinIO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

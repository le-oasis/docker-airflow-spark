{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35988bb8-fb16-41ea-91bc-4a71b6d55ebb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "import os\n",
    "import sys\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import *\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import from_unixtime, col, to_timestamp\n",
    "from pyspark.sql.functions import udf\n",
    "import hashlib\n",
    "import urllib.request\n",
    "import json\n",
    "from datetime import timedelta, date\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SQLContext\n",
    "from itertools import islice\n",
    "from pyspark.sql.functions import col\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d2fc31-a886-4315-8ce6-3e7828a0589f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bec551ea-4f18-404b-a8d4-a7a16a05f771",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "import os\n",
    "\n",
    "\n",
    "def download_path(filename):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        filename: name of the file located in s3 bucket that will be downloaded\n",
    "    \n",
    "    output:\n",
    "        downloads a specific filename from s3 and saves it to a local directory called data\n",
    "        for staging\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "    bucket_name = 'd2b-internal-assessment-bucket'\n",
    "    response = s3.list_objects(Bucket=bucket_name, Prefix=\"orders_data\")\n",
    "    # specify directory to save downloaded files\n",
    "    local = '/home/jovyan/work/data'\n",
    "    if not os.path.exists(local):\n",
    "       os.makedirs(local)\n",
    "\n",
    "    s3.download_file(bucket_name, \"orders_data/{}\".format(filename), local + \"/\" \"{}\".format(filename))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_path(\"orders.csv\")\n",
    "    download_path(\"reviews.csv\")\n",
    "    download_path(\"shipment_deliveries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e211b5b4-3b30-431a-8adf-2e7d00f3645a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5accfa-e7a0-45c6-a362-45b871a7a130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a42ed8-ea7a-410c-89c4-de9d890c8af4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7462afff-4f11-48e2-bcde-87876e4f20b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8faf49d-f930-4bcd-a04b-3799d8cd664c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.apache.hadoop#hadoop-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-edc9e336-abc1-403a-aa31-40bca1bd4320;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.0 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.375 in central\n",
      "\tfound org.apache.hadoop#hadoop-client;3.2.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-common;3.2.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.2.0 in central\n",
      "\tfound commons-cli#commons-cli;1.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.1.1 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.2 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.11 in central\n",
      "\tfound commons-io#commons-io;2.5 in central\n",
      "\tfound commons-net#commons-net;3.6 in central\n",
      "\tfound commons-collections#commons-collections;3.2.2 in central\n",
      "\tfound org.eclipse.jetty#jetty-servlet;9.3.24.v20180605 in central\n",
      "\tfound org.eclipse.jetty#jetty-security;9.3.24.v20180605 in central\n",
      "\tfound org.eclipse.jetty#jetty-webapp;9.3.24.v20180605 in central\n",
      "\tfound org.eclipse.jetty#jetty-xml;9.3.24.v20180605 in central\n",
      "\tfound com.sun.jersey#jersey-servlet;1.19 in central\n",
      "\tfound log4j#log4j;1.2.17 in central\n",
      "\tfound commons-beanutils#commons-beanutils;1.9.3 in central\n",
      "\tfound org.apache.commons#commons-configuration2;2.1.1 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.7 in central\n",
      "\tfound org.apache.commons#commons-text;1.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.25 in central\n",
      "\tfound org.apache.avro#avro;1.7.7 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.0.5 in central\n",
      "\tfound org.apache.commons#commons-compress;1.4.1 in central\n",
      "\tfound org.tukaani#xz;1.0 in central\n",
      "\tfound com.google.re2j#re2j;1.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound com.google.code.gson#gson;2.2.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-auth;3.2.0 in central\n",
      "\tfound com.nimbusds#nimbus-jose-jwt;4.41.1 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound net.minidev#json-smart;2.3 in central\n",
      "\tfound net.minidev#accessors-smart;1.2 in central\n",
      "\tfound org.ow2.asm#asm;5.0.4 in central\n",
      "\tfound org.apache.curator#curator-framework;2.12.0 in central\n",
      "\tfound org.apache.curator#curator-client;2.12.0 in central\n",
      "\tfound org.apache.kerby#kerb-simplekdc;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-client;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-config;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-core;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-pkix;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-asn1;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-common;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-crypto;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#token-provider;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-admin;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-server;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-identity;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-xdr;1.0.1 in central\n",
      "\tfound org.apache.curator#curator-recipes;2.12.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.9.5 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.9.5 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.9.5 in central\n",
      "\tfound org.codehaus.woodstox#stax2-api;3.1.4 in central\n",
      "\tfound com.fasterxml.woodstox#woodstox-core;5.0.3 in central\n",
      "\tfound dnsjava#dnsjava;2.1.7 in central\n",
      "\tfound javax.servlet.jsp#jsp-api;2.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-hdfs-client;3.2.0 in central\n",
      "\tfound com.squareup.okhttp#okhttp;2.7.5 in central\n",
      "\tfound com.squareup.okio#okio;1.6.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-api;3.2.0 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.11 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-client;3.2.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-mapreduce-client-core;3.2.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-common;3.2.0 in central\n",
      "\tfound javax.servlet#javax.servlet-api;3.1.0 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.3.24.v20180605 in central\n",
      "\tfound com.sun.jersey#jersey-core;1.19 in central\n",
      "\tfound javax.ws.rs#jsr311-api;1.1.1 in central\n",
      "\tfound com.sun.jersey#jersey-client;1.19 in central\n",
      "\tfound com.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.9.5 in central\n",
      "\tfound com.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.9.5 in central\n",
      "\tfound com.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.9.5 in central\n",
      "\tfound org.apache.hadoop#hadoop-mapreduce-client-jobclient;3.2.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-mapreduce-client-common;3.2.0 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.0/hadoop-aws-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.2.0!hadoop-aws.jar (40ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client/3.2.0/hadoop-client-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client;3.2.0!hadoop-client.jar (8ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.375/aws-java-sdk-bundle-1.11.375.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.11.375!aws-java-sdk-bundle.jar (568ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/3.2.0/hadoop-common-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-common;3.2.0!hadoop-common.jar (19ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-hdfs-client/3.2.0/hadoop-hdfs-client-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-hdfs-client;3.2.0!hadoop-hdfs-client.jar (22ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-api/3.2.0/hadoop-yarn-api-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-yarn-api;3.2.0!hadoop-yarn-api.jar (16ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-client/3.2.0/hadoop-yarn-client-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-yarn-client;3.2.0!hadoop-yarn-client.jar (7ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-core/3.2.0/hadoop-mapreduce-client-core-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-mapreduce-client-core;3.2.0!hadoop-mapreduce-client-core.jar (31ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/3.2.0/hadoop-mapreduce-client-jobclient-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-mapreduce-client-jobclient;3.2.0!hadoop-mapreduce-client-jobclient.jar (6ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-annotations/3.2.0/hadoop-annotations-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-annotations;3.2.0!hadoop-annotations.jar (6ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpclient/4.5.2/httpclient-4.5.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.httpcomponents#httpclient;4.5.2!httpclient.jar (12ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-codec/commons-codec/1.11/commons-codec-1.11.jar ...\n",
      "\t[SUCCESSFUL ] commons-codec#commons-codec;1.11!commons-codec.jar (9ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-net/commons-net/3.6/commons-net-3.6.jar ...\n",
      "\t[SUCCESSFUL ] commons-net#commons-net;3.6!commons-net.jar (8ms)\n",
      "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-servlet/9.3.24.v20180605/jetty-servlet-9.3.24.v20180605.jar ...\n",
      "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-servlet;9.3.24.v20180605!jetty-servlet.jar (6ms)\n",
      "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-webapp/9.3.24.v20180605/jetty-webapp-9.3.24.v20180605.jar ...\n",
      "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-webapp;9.3.24.v20180605!jetty-webapp.jar (8ms)\n",
      "downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-servlet/1.19/jersey-servlet-1.19.jar ...\n",
      "\t[SUCCESSFUL ] com.sun.jersey#jersey-servlet;1.19!jersey-servlet.jar (31ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar ...\n",
      "\t[SUCCESSFUL ] commons-beanutils#commons-beanutils;1.9.3!commons-beanutils.jar (10ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-configuration2/2.1.1/commons-configuration2-2.1.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-configuration2;2.1.1!commons-configuration2.jar (12ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.7/commons-lang3-3.7.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-lang3;3.7!commons-lang3.jar (9ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-text/1.4/commons-text-1.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-text;1.4!commons-text.jar (8ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.25!slf4j-api.jar (6ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/re2j/re2j/1.1/re2j-1.1.jar ...\n",
      "\t[SUCCESSFUL ] com.google.re2j#re2j;1.1!re2j.jar (8ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-auth/3.2.0/hadoop-auth-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-auth;3.2.0!hadoop-auth.jar (7ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/curator/curator-client/2.12.0/curator-client-2.12.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.curator#curator-client;2.12.0!curator-client.jar(bundle) (17ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/curator/curator-recipes/2.12.0/curator-recipes-2.12.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.curator#curator-recipes;2.12.0!curator-recipes.jar(bundle) (8ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (6ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-compress;1.4.1!commons-compress.jar (8ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-simplekdc/1.0.1/kerb-simplekdc-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerb-simplekdc;1.0.1!kerb-simplekdc.jar (6ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.9.5/jackson-databind-2.9.5.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-databind;2.9.5!jackson-databind.jar(bundle) (12ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/woodstox/stax2-api/3.1.4/stax2-api-3.1.4.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.woodstox#stax2-api;3.1.4!stax2-api.jar(bundle) (7ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/woodstox/woodstox-core/5.0.3/woodstox-core-5.0.3.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.woodstox#woodstox-core;5.0.3!woodstox-core.jar(bundle) (8ms)\n",
      "downloading https://repo1.maven.org/maven2/dnsjava/dnsjava/2.1.7/dnsjava-2.1.7.jar ...\n",
      "\t[SUCCESSFUL ] dnsjava#dnsjava;2.1.7!dnsjava.jar (8ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcore/4.4.4/httpcore-4.4.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.httpcomponents#httpcore;4.4.4!httpcore.jar (8ms)\n",
      "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-security/9.3.24.v20180605/jetty-security-9.3.24.v20180605.jar ...\n",
      "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-security;9.3.24.v20180605!jetty-security.jar (6ms)\n",
      "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-xml/9.3.24.v20180605/jetty-xml-9.3.24.v20180605.jar ...\n",
      "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-xml;9.3.24.v20180605!jetty-xml.jar (6ms)\n",
      "downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.0/xz-1.0.jar ...\n",
      "\t[SUCCESSFUL ] org.tukaani#xz;1.0!xz.jar (6ms)\n",
      "downloading https://repo1.maven.org/maven2/com/nimbusds/nimbus-jose-jwt/4.41.1/nimbus-jose-jwt-4.41.1.jar ...\n",
      "\t[SUCCESSFUL ] com.nimbusds#nimbus-jose-jwt;4.41.1!nimbus-jose-jwt.jar (7ms)\n",
      "downloading https://repo1.maven.org/maven2/net/minidev/json-smart/2.3/json-smart-2.3.jar ...\n",
      "\t[SUCCESSFUL ] net.minidev#json-smart;2.3!json-smart.jar(bundle) (6ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/curator/curator-framework/2.12.0/curator-framework-2.12.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.curator#curator-framework;2.12.0!curator-framework.jar(bundle) (7ms)\n",
      "downloading https://repo1.maven.org/maven2/net/minidev/accessors-smart/1.2/accessors-smart-1.2.jar ...\n",
      "\t[SUCCESSFUL ] net.minidev#accessors-smart;1.2!accessors-smart.jar(bundle) (6ms)\n",
      "downloading https://repo1.maven.org/maven2/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar ...\n",
      "\t[SUCCESSFUL ] org.ow2.asm#asm;5.0.4!asm.jar (6ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-client/1.0.1/kerb-client-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerb-client;1.0.1!kerb-client.jar (7ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-admin/1.0.1/kerb-admin-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerb-admin;1.0.1!kerb-admin.jar (10ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerby-config/1.0.1/kerby-config-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerby-config;1.0.1!kerby-config.jar (6ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-core/1.0.1/kerb-core-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerb-core;1.0.1!kerb-core.jar (9ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-common/1.0.1/kerb-common-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerb-common;1.0.1!kerb-common.jar (6ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-util/1.0.1/kerb-util-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerb-util;1.0.1!kerb-util.jar (17ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/token-provider/1.0.1/token-provider-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#token-provider;1.0.1!token-provider.jar (6ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerby-pkix/1.0.1/kerby-pkix-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerby-pkix;1.0.1!kerby-pkix.jar (9ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerby-asn1/1.0.1/kerby-asn1-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerby-asn1;1.0.1!kerby-asn1.jar (7ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerby-util/1.0.1/kerby-util-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerby-util;1.0.1!kerby-util.jar (6ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-crypto/1.0.1/kerb-crypto-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerb-crypto;1.0.1!kerb-crypto.jar (8ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-server/1.0.1/kerb-server-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerb-server;1.0.1!kerb-server.jar (9ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerby-xdr/1.0.1/kerby-xdr-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerby-xdr;1.0.1!kerby-xdr.jar (6ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-identity/1.0.1/kerb-identity-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerb-identity;1.0.1!kerb-identity.jar (6ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.9.5/jackson-annotations-2.9.5.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-annotations;2.9.5!jackson-annotations.jar(bundle) (6ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.9.5/jackson-core-2.9.5.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-core;2.9.5!jackson-core.jar(bundle) (9ms)\n",
      "downloading https://repo1.maven.org/maven2/javax/xml/bind/jaxb-api/2.2.11/jaxb-api-2.2.11.jar ...\n",
      "\t[SUCCESSFUL ] javax.xml.bind#jaxb-api;2.2.11!jaxb-api.jar (6ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-common/3.2.0/hadoop-yarn-common-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-yarn-common;3.2.0!hadoop-yarn-common.jar (14ms)\n",
      "downloading https://repo1.maven.org/maven2/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar ...\n",
      "\t[SUCCESSFUL ] javax.servlet#javax.servlet-api;3.1.0!javax.servlet-api.jar (6ms)\n",
      "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/9.3.24.v20180605/jetty-util-9.3.24.v20180605.jar ...\n",
      "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-util;9.3.24.v20180605!jetty-util.jar (9ms)\n",
      "downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-core/1.19/jersey-core-1.19.jar ...\n",
      "\t[SUCCESSFUL ] com.sun.jersey#jersey-core;1.19!jersey-core.jar (8ms)\n",
      "downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-client/1.19/jersey-client-1.19.jar ...\n",
      "\t[SUCCESSFUL ] com.sun.jersey#jersey-client;1.19!jersey-client.jar (6ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.9.5/jackson-module-jaxb-annotations-2.9.5.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.9.5!jackson-module-jaxb-annotations.jar(bundle) (6ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.9.5/jackson-jaxrs-json-provider-2.9.5.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.9.5!jackson-jaxrs-json-provider.jar(bundle) (6ms)\n",
      "downloading https://repo1.maven.org/maven2/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1.jar ...\n",
      "\t[SUCCESSFUL ] javax.ws.rs#jsr311-api;1.1.1!jsr311-api.jar (7ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.9.5/jackson-jaxrs-base-2.9.5.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.9.5!jackson-jaxrs-base.jar(bundle) (6ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-common/3.2.0/hadoop-mapreduce-client-common-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-mapreduce-client-common;3.2.0!hadoop-mapreduce-client-common.jar (10ms)\n",
      ":: resolution report :: resolve 24593ms :: artifacts dl 1250ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.375 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.9.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.9.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.9.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.9.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.9.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.9.5 from central in [default]\n",
      "\tcom.fasterxml.woodstox#woodstox-core;5.0.3 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.2.4 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.1 from central in [default]\n",
      "\tcom.nimbusds#nimbus-jose-jwt;4.41.1 from central in [default]\n",
      "\tcom.squareup.okhttp#okhttp;2.7.5 from central in [default]\n",
      "\tcom.squareup.okio#okio;1.6.0 from central in [default]\n",
      "\tcom.sun.jersey#jersey-client;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-core;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-servlet;1.19 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.3 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.9.3 from central in [default]\n",
      "\tcommons-cli#commons-cli;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.11 from central in [default]\n",
      "\tcommons-collections#commons-collections;3.2.2 from central in [default]\n",
      "\tcommons-io#commons-io;2.5 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tcommons-net#commons-net;3.6 from central in [default]\n",
      "\tdnsjava#dnsjava;2.1.7 from central in [default]\n",
      "\tjavax.servlet#javax.servlet-api;3.1.0 from central in [default]\n",
      "\tjavax.servlet.jsp#jsp-api;2.1 from central in [default]\n",
      "\tjavax.ws.rs#jsr311-api;1.1.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.11 from central in [default]\n",
      "\tlog4j#log4j;1.2.17 from central in [default]\n",
      "\tnet.minidev#accessors-smart;1.2 from central in [default]\n",
      "\tnet.minidev#json-smart;2.3 from central in [default]\n",
      "\torg.apache.avro#avro;1.7.7 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.4.1 from central in [default]\n",
      "\torg.apache.commons#commons-configuration2;2.1.1 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.7 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.1.1 from central in [default]\n",
      "\torg.apache.commons#commons-text;1.4 from central in [default]\n",
      "\torg.apache.curator#curator-client;2.12.0 from central in [default]\n",
      "\torg.apache.curator#curator-framework;2.12.0 from central in [default]\n",
      "\torg.apache.curator#curator-recipes;2.12.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-auth;3.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client;3.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-common;3.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-hdfs-client;3.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-mapreduce-client-common;3.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-mapreduce-client-core;3.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-mapreduce-client-jobclient;3.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-api;3.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-client;3.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-common;3.2.0 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.2 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.4 from central in [default]\n",
      "\torg.apache.kerby#kerb-admin;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-client;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-common;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-core;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-crypto;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-identity;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-server;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-simplekdc;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-asn1;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-config;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-pkix;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-xdr;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#token-provider;1.0.1 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.woodstox#stax2-api;3.1.4 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-security;9.3.24.v20180605 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-servlet;9.3.24.v20180605 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.3.24.v20180605 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-webapp;9.3.24.v20180605 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-xml;9.3.24.v20180605 from central in [default]\n",
      "\torg.ow2.asm#asm;5.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.25 from central in [default]\n",
      "\torg.tukaani#xz;1.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.0.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   86  |   68  |   68  |   0   ||   86  |   68  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-edc9e336-abc1-403a-aa31-40bca1bd4320\n",
      "\tconfs: [default]\n",
      "\t68 artifacts copied, 18 already retrieved (127526kB/391ms)\n",
      "23/04/24 14:37:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/24 14:37:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/04/24 14:37:21 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.\n",
      "java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension\n",
      "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n",
      "\tat java.base/java.lang.Class.forName0(Native Method)\n",
      "\tat java.base/java.lang.Class.forName(Class.java:398)\n",
      "\tat org.apache.spark.util.Utils$.classForName(Utils.scala:216)\n",
      "\tat org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$1(SparkSession.scala:1194)\n",
      "\tat org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$1$adapted(SparkSession.scala:1192)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1192)\n",
      "\tat org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:104)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/04/24 14:37:22 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3://d2b-internal-assessment-bucket/orders_data/orders.csv.\n",
      "org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o52.load.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:747)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:745)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:577)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      3\u001b[0m spark \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      4\u001b[0m     SparkSession\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mbuilder\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Replace \"s3://d2b-internal-assessment-bucket\" with the path to your S3 bucket\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3://d2b-internal-assessment-bucket/orders_data/orders.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:158\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o52.load.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:747)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:745)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:577)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.0,org.apache.hadoop:hadoop-client:3.2.0\")\n",
    "    .config(\"spark.jars.excludes\", \"com.google.guava:guava\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Replace \"s3://d2b-internal-assessment-bucket\" with the path to your S3 bucket\n",
    "df = spark.read.format(\"csv\").option(\"header\", True).load(\"s3://d2b-internal-assessment-bucket/orders_data/orders.csv\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71a25207-0aaf-42e5-b153-e2424bebf313",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/24 14:29:09 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3://d2b-internal-assessment-bucket/orders_data/orders.csv.\n",
      "org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n",
      "\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:571)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o133.csv.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:747)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:745)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:577)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:571)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morders_data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m table_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morders\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 23\u001b[0m orders_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m(\u001b[49m\u001b[43morders_schema\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbucket_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtable_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:410\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    408\u001b[0m     path \u001b[38;5;241m=\u001b[39m [path]\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o133.csv.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:747)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:745)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:577)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:571)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Define schema for orders data\n",
    "orders_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", DateType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"unit_price\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"total_price\", IntegerType(), True)\n",
    "])\n",
    "################################################################################\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"S3ToPostgres\").getOrCreate()\n",
    "################################################################################\n",
    "# Set AWS credentials\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "################################################################################\n",
    "# Read orders data from S3\n",
    "bucket_name = \"d2b-internal-assessment-bucket\"\n",
    "prefix = \"orders_data\"\n",
    "table_name = \"orders\"\n",
    "orders_df = spark.read.schema(orders_schema).csv(f\"s3://{bucket_name}/{prefix}/{table_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe6a2d8-91c3-4e26-8d86-de4205023555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

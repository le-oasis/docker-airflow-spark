{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "316497fd-9813-4f22-88ba-88ac3adb9efe",
   "metadata": {},
   "source": [
    "## Import the modules \n",
    "\n",
    "- In this scenario, we are going to import the pyspark and pyspark SQL modules and create a spark session as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ad05b13-ffd0-4a76-8228-bebc40dbeca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d770cd1-2a67-4ad2-9ce5-d581b1ee8af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Session\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.jars\", \"/home/jovyan/work/jars/postgresql-42.4.0.jar\") \\\n",
    "    .master(\"local\").appName(\"PySpark-Postgres\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d65c1b-a66c-4fc9-b530-631103c9e7d3",
   "metadata": {},
   "source": [
    "## Read Data from the table\n",
    "- Here we are going to read the data table from PostgreSQL and create the DataFrames. \n",
    "- To read the data frame, we will read() method through the JDBC URL and provide the PostgreSQL jar Driver path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab18c481-12c7-4972-a0e6-e6c6e7d61052",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the Oasis table we created from our DAG.\n",
    "## Using the 'Test' database we created upon init \n",
    "## User 'airflow' as we defined in our .yaml\n",
    "\n",
    "df = spark.read.format(\"jdbc\").option(\"url\", \"jdbc:postgresql://oasispostgres:5432/metastore\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"customers\") \\\n",
    "    .option(\"user\", \"hive\").option(\"password\", \"hive\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c561df-d3e2-4289-9804-332669501f13",
   "metadata": {},
   "source": [
    "## To view the Schema\n",
    "- Here we will read the schema of the stored table as a dataframe, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fefd7a18-9f53-4b19-bb64-bc4c0431fd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- created: timestamp (nullable = true)\n",
      " |-- updated: timestamp (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d704939f-1036-4b8b-9919-efad866b62fe",
   "metadata": {},
   "source": [
    "## To view the content of the table\n",
    "- Here we are going to read the content of the table as a dataframe. \n",
    "- We will print the top 5 rows from the dataframe as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23131c90-84b1-426b-8e4a-b3b0d83d97ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+--------------------+----------+---------+--------------------+\n",
      "| id|            created|             updated|first_name|last_name|               email|\n",
      "+---+-------------------+--------------------+----------+---------+--------------------+\n",
      "|  1|2021-02-16 00:16:06|2022-07-29 11:58:...|     Scott|   Haines|  scott@coffeeco.com|\n",
      "|  2|2021-02-16 00:16:06|2022-07-29 11:58:...|      John|     Hamm|  john.hamm@acme.com|\n",
      "|  3|2021-02-16 00:16:06|2022-07-29 11:58:...|      Milo|   Haines|mhaines@coffeeco.com|\n",
      "|  4|2021-02-21 21:00:00|2022-07-29 11:58:...|     Penny|   Haines|  penny@coffeeco.com|\n",
      "|  5|2021-02-21 22:00:00|2022-07-29 11:58:...|     Cloud|     Fast| cloud.fast@acme.com|\n",
      "+---+-------------------+--------------------+----------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d9d84-ef73-460d-b7da-cba11e3ab5f4",
   "metadata": {},
   "source": [
    "- Interacting with a JDBC Backed DataFrame Abstracts Away the Complexities of Connecting to a Remote (External) RDBMS\n",
    "- The results of calling show on the JDBC backed DataFrame yields the first three entries in the customers table from your PostgresSQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2222a390-f6d8-4faf-a8a9-ceca05f682df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----------+--------------------+\n",
      "|             updated| id|first_name|               email|\n",
      "+--------------------+---+----------+--------------------+\n",
      "|2022-07-29 11:58:...|  1|     Scott|  scott@coffeeco.com|\n",
      "|2022-07-29 11:58:...|  2|      John|  john.hamm@acme.com|\n",
      "|2022-07-29 11:58:...|  3|      Milo|mhaines@coffeeco.com|\n",
      "|2022-07-29 11:58:...|  4|     Penny|  penny@coffeeco.com|\n",
      "|2022-07-29 11:58:...|  5|     Cloud| cloud.fast@acme.com|\n",
      "|2022-07-29 11:58:...|  6|   Marshal|   paws@coffeeco.com|\n",
      "|2022-07-29 11:58:...|  7|    Willow| willow@coffeeco.com|\n",
      "|2022-07-29 11:58:...|  8|    Clover|    pup@coffeeco.com|\n",
      "+--------------------+---+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"updated\",\"id\", \"first_name\", \"email\").limit(8).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85773d67-b27e-4ccb-a7e5-7da2f8661348",
   "metadata": {},
   "source": [
    "## Describing Views and Tables\n",
    "\n",
    "- You have learned to use JDBC to connect to your PostgreSQL docker container. \n",
    "- Wouldn’t it make sense that we could describe the schema of the table by doing a simple SQL style describe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "782fb18f-7a1d-48f2-bb75-4e3fddc84c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|  col_name|data_type|comment|\n",
      "+----------+---------+-------+\n",
      "|        id|      int|   null|\n",
      "|   created|timestamp|   null|\n",
      "|   updated|timestamp|   null|\n",
      "|first_name|   string|   null|\n",
      "| last_name|   string|   null|\n",
      "|     email|   string|   null|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"customers\")\n",
    "spark.sql(\"desc customers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef77959-1d13-4a97-987e-09b9a8aacc77",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "- Here we learned to read data from PostgreSQL in Pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c18d960-5f19-4bb7-8a38-372c141a7f83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

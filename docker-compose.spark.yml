version: "3.7"
services:
   
   # Postgres used by airflow
    postgres:
        hostname: mypostgres
        container_name: postgres_container
        image: postgres:13
        environment:
            POSTGRES_USER: airflow
            POSTGRES_PASSWORD: airflow
            POSTGRES_DB: airflow
        volumes:
            - postgres-db-volume:/var/lib/postgresql/data
        ports:
            - "5432:5432"
        networks:
            - default_net
        restart: always
        healthcheck:
            test: ["CMD", "pg_isready", "-U", "airflow"]
            interval: 5s
            retries: 5
            

    # Airflow Webserver
    airflow-webserver:
        hostname: myairflow
        container_name: airflow_container
        image: prunedge
        environment:
            - LOAD_EX=n
            - EXECUTOR=Local
        volumes:
            - airflow-data:/usr/local/airflow/data
            - ./airflow/logs:/usr/local/airflow/logs
            - ./airflow/dags:/usr/local/airflow/dags
            # - ./airflow/requirements/requirements.txt:/requirements.txt
            # - ./dags:/usr/local/airflow/dags #DAG folder
            - ./spark/app:/usr/local/spark/app #Spark Scripts (Must be the same path in airflow and Spark Cluster)
            - ./spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)
        ports:
            - "8080:8080"
        networks:
            - default_net
        depends_on:
            - postgres
        restart: always
        command: webserver
        healthcheck:
            test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
            interval: 30s
            timeout: 30s
            retries: 3

    # Minio for object storage
    minio:
        hostname: myminio
        container_name: minio_container
        image: 'bitnami/minio:2021' # latest image as of 2021-11-08
        environment:
            MINIO_ACCESS_KEY: minio_admin
            MINIO_SECRET_KEY: minio_password
        ports:
            - '9000:9000'
            - '9001:9001'
        volumes:
            - './minio/data:/data'
        networks:
            - default_net
        healthcheck:
            test: ["CMD", "curl", "-f", "http://myminio:9000/minio/health/live"]
            interval: 30s
            timeout: 20s
            retries: 3
        

    # Spark with 3 workers
    spark:
        hostname: spark
        container_name: spark_container
        image: bitnami/spark:3.1.2
        user: root # Run container as root container: https://docs.bitnami.com/tutorials/work-with-non-root-containers/
        networks:
            - default_net
        environment:
            - SPARK_MODE=master
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        volumes:
            - ./spark/app:/usr/local/spark/app # Spark scripts folder (Must be the same path in airflow and Spark Cluster)
            - ./spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)
        ports:
            - "8181:8080"
            - "7077:7077"

    spark-worker-1:
        hostname: spark-worker-1
        container_name: spark-worker-1-container
        image: bitnami/spark:3.1.2
        user: root
        networks:
            - default_net
        environment:
            - SPARK_MODE=worker
            - SPARK_MASTER_URL=spark://spark:7077
            - SPARK_WORKER_MEMORY=1G
            - SPARK_WORKER_CORES=1
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        volumes:
            - ./spark/app:/usr/local/spark/app # Spark scripts folder (Must be the same path in airflow and Spark Cluster)
            - ./spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)

    spark-worker-2:
        hostname: spark-worker-2
        container_name: spark-worker-2-container
        image: bitnami/spark:3.1.2
        user: root
        networks:
            - default_net
        environment:
            - SPARK_MODE=worker
            - SPARK_MASTER_URL=spark://spark:7077
            - SPARK_WORKER_MEMORY=1G
            - SPARK_WORKER_CORES=1
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        volumes:
            - ./spark/app:/usr/local/spark/app # Spark scripts folder (Must be the same path in airflow and Spark Cluster)
            - ./spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)

    spark-worker-3:
        hostname: spark-worker-3
        container_name: spark-worker-3-container
        image: bitnami/spark:3.1.2
        user: root
        networks:
            - default_net
        environment:
            - SPARK_MODE=worker
            - SPARK_MASTER_URL=spark://spark:7077
            - SPARK_WORKER_MEMORY=1G
            - SPARK_WORKER_CORES=1
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        volumes:
            - ./spark/app:/usr/local/spark/app # Spark scripts folder (Must be the same path in airflow and Spark Cluster)
            - ./spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)

    # Jupyter notebook
    jupyter-spark:
        hostname: jupyter-spark
        container_name: jupyter-spark-container
        image: jupyter/pyspark-notebook:spark-3.1.2
        networks:
            - default_net
        ports:
          - "8888:8888"
          - "4040-4080:4040-4080"
        volumes:
          - ./notebooks:/home/jovyan/work/notebooks/
          - ./spark/resources/data:/home/jovyan/work/data/
          - ./spark/resources/jars:/home/jovyan/work/jars/

volumes:
    airflow-data:
    postgres-db-volume:


networks:
    default_net:
        driver: bridge
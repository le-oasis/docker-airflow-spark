[2022-07-06 07:28:57,970] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: spark-test.spark_job manual__2022-07-06T07:28:52.009617+00:00 [queued]>
[2022-07-06 07:28:58,046] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: spark-test.spark_job manual__2022-07-06T07:28:52.009617+00:00 [queued]>
[2022-07-06 07:28:58,053] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2022-07-06 07:28:58,057] {taskinstance.py:1239} INFO - Starting attempt 1 of 2
[2022-07-06 07:28:58,060] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2022-07-06 07:28:58,136] {taskinstance.py:1259} INFO - Executing <Task(SparkSubmitOperator): spark_job> on 2022-07-06 07:28:52.009617+00:00
[2022-07-06 07:28:58,161] {standard_task_runner.py:52} INFO - Started process 52 to run task
[2022-07-06 07:28:58,210] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'spark-test', 'spark_job', 'manual__2022-07-06T07:28:52.009617+00:00', '--job-id', '5', '--raw', '--subdir', 'DAGS_FOLDER/spark-test.py', '--cfg-path', '/tmp/tmpkwktm9yc', '--error-file', '/tmp/tmpgdfkrkei']
[2022-07-06 07:28:58,273] {standard_task_runner.py:77} INFO - Job 5: Subtask spark_job
[2022-07-06 07:29:02,602] {logging_mixin.py:109} INFO - Running <TaskInstance: spark-test.spark_job manual__2022-07-06T07:28:52.009617+00:00 [running]> on host 538ddc85acbc
[2022-07-06 07:29:03,018] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=***@***.com
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=spark-test
AIRFLOW_CTX_TASK_ID=spark_job
AIRFLOW_CTX_EXECUTION_DATE=2022-07-06T07:28:52.009617+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2022-07-06T07:28:52.009617+00:00
[2022-07-06 07:29:03,085] {base.py:79} INFO - Using connection to: id: spark_connection. Host: spark://spark, Port: 7077, Schema: , Login: , Password: None, extra: {'queue': 'root.default'}
[2022-07-06 07:29:03,104] {spark_submit.py:334} INFO - Spark-Submit cmd: spark-submit --master spark://spark:7077 --conf spark.master=spark://spark:7077 --name Spark Hello World --verbose --queue root.default /usr/local/spark/app/hello-world.py /usr/local/spark/resources/data/***.cfg
[2022-07-06 07:29:32,767] {spark_submit.py:485} INFO - Using properties file: null
[2022-07-06 07:29:33,364] {spark_submit.py:485} INFO - WARNING: An illegal reflective access operation has occurred
[2022-07-06 07:29:33,366] {spark_submit.py:485} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/spark/spark-3.0.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2022-07-06 07:29:33,368] {spark_submit.py:485} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2022-07-06 07:29:33,370] {spark_submit.py:485} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2022-07-06 07:29:33,372] {spark_submit.py:485} INFO - WARNING: All illegal access operations will be denied in a future release
[2022-07-06 07:29:33,564] {spark_submit.py:485} INFO - Parsed arguments:
[2022-07-06 07:29:33,566] {spark_submit.py:485} INFO - master                  spark://spark:7077
[2022-07-06 07:29:33,568] {spark_submit.py:485} INFO - deployMode              null
[2022-07-06 07:29:33,569] {spark_submit.py:485} INFO - executorMemory          null
[2022-07-06 07:29:33,572] {spark_submit.py:485} INFO - executorCores           null
[2022-07-06 07:29:33,575] {spark_submit.py:485} INFO - totalExecutorCores      null
[2022-07-06 07:29:33,578] {spark_submit.py:485} INFO - propertiesFile          null
[2022-07-06 07:29:33,580] {spark_submit.py:485} INFO - driverMemory            null
[2022-07-06 07:29:33,582] {spark_submit.py:485} INFO - driverCores             null
[2022-07-06 07:29:33,585] {spark_submit.py:485} INFO - driverExtraClassPath    null
[2022-07-06 07:29:33,587] {spark_submit.py:485} INFO - driverExtraLibraryPath  null
[2022-07-06 07:29:33,590] {spark_submit.py:485} INFO - driverExtraJavaOptions  null
[2022-07-06 07:29:33,593] {spark_submit.py:485} INFO - supervise               false
[2022-07-06 07:29:33,595] {spark_submit.py:485} INFO - queue                   root.default
[2022-07-06 07:29:33,599] {spark_submit.py:485} INFO - numExecutors            null
[2022-07-06 07:29:33,607] {spark_submit.py:485} INFO - files                   null
[2022-07-06 07:29:33,609] {spark_submit.py:485} INFO - pyFiles                 null
[2022-07-06 07:29:33,611] {spark_submit.py:485} INFO - archives                null
[2022-07-06 07:29:33,613] {spark_submit.py:485} INFO - mainClass               null
[2022-07-06 07:29:33,614] {spark_submit.py:485} INFO - primaryResource         file:/usr/local/spark/app/hello-world.py
[2022-07-06 07:29:33,616] {spark_submit.py:485} INFO - name                    Spark Hello World
[2022-07-06 07:29:33,619] {spark_submit.py:485} INFO - childArgs               [/usr/local/spark/resources/data/***.cfg]
[2022-07-06 07:29:33,621] {spark_submit.py:485} INFO - jars                    null
[2022-07-06 07:29:33,627] {spark_submit.py:485} INFO - packages                null
[2022-07-06 07:29:33,629] {spark_submit.py:485} INFO - packagesExclusions      null
[2022-07-06 07:29:33,631] {spark_submit.py:485} INFO - repositories            null
[2022-07-06 07:29:33,633] {spark_submit.py:485} INFO - verbose                 true
[2022-07-06 07:29:33,636] {spark_submit.py:485} INFO - 
[2022-07-06 07:29:33,639] {spark_submit.py:485} INFO - Spark properties used, including those specified through
[2022-07-06 07:29:33,640] {spark_submit.py:485} INFO - --conf and those from the properties file null:
[2022-07-06 07:29:33,642] {spark_submit.py:485} INFO - (spark.master,spark://spark:7077)
[2022-07-06 07:29:33,644] {spark_submit.py:485} INFO - 
[2022-07-06 07:29:33,646] {spark_submit.py:485} INFO - 
[2022-07-06 07:29:35,197] {spark_submit.py:485} INFO - 22/07/06 07:29:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-07-06 07:29:38,129] {spark_submit.py:485} INFO - Main class:
[2022-07-06 07:29:38,142] {spark_submit.py:485} INFO - org.apache.spark.deploy.PythonRunner
[2022-07-06 07:29:38,144] {spark_submit.py:485} INFO - Arguments:
[2022-07-06 07:29:38,146] {spark_submit.py:485} INFO - file:/usr/local/spark/app/hello-world.py
[2022-07-06 07:29:38,152] {spark_submit.py:485} INFO - null
[2022-07-06 07:29:38,153] {spark_submit.py:485} INFO - /usr/local/spark/resources/data/***.cfg
[2022-07-06 07:29:38,156] {spark_submit.py:485} INFO - Spark config:
[2022-07-06 07:29:38,157] {spark_submit.py:485} INFO - (spark.master,spark://spark:7077)
[2022-07-06 07:29:38,159] {spark_submit.py:485} INFO - (spark.app.name,Spark Hello World)
[2022-07-06 07:29:38,161] {spark_submit.py:485} INFO - (spark.submit.pyFiles,)
[2022-07-06 07:29:38,162] {spark_submit.py:485} INFO - (spark.submit.deployMode,client)
[2022-07-06 07:29:38,165] {spark_submit.py:485} INFO - Classpath elements:
[2022-07-06 07:29:38,168] {spark_submit.py:485} INFO - 
[2022-07-06 07:29:38,169] {spark_submit.py:485} INFO - 
[2022-07-06 07:29:38,171] {spark_submit.py:485} INFO - 
[2022-07-06 07:29:41,842] {spark_submit.py:485} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-07-06 07:29:41,889] {spark_submit.py:485} INFO - 22/07/06 07:29:41 INFO SparkContext: Running Spark version 3.0.1
[2022-07-06 07:29:42,013] {spark_submit.py:485} INFO - 22/07/06 07:29:42 INFO ResourceUtils: ==============================================================
[2022-07-06 07:29:42,019] {spark_submit.py:485} INFO - 22/07/06 07:29:42 INFO ResourceUtils: Resources for spark.driver:
[2022-07-06 07:29:42,021] {spark_submit.py:485} INFO - 
[2022-07-06 07:29:42,025] {spark_submit.py:485} INFO - 22/07/06 07:29:42 INFO ResourceUtils: ==============================================================
[2022-07-06 07:29:42,035] {spark_submit.py:485} INFO - 22/07/06 07:29:42 INFO SparkContext: Submitted application: Spark Hello World
[2022-07-06 07:29:42,307] {spark_submit.py:485} INFO - 22/07/06 07:29:42 INFO SecurityManager: Changing view acls to: default
[2022-07-06 07:29:42,310] {spark_submit.py:485} INFO - 22/07/06 07:29:42 INFO SecurityManager: Changing modify acls to: default
[2022-07-06 07:29:42,315] {spark_submit.py:485} INFO - 22/07/06 07:29:42 INFO SecurityManager: Changing view acls groups to:
[2022-07-06 07:29:42,316] {spark_submit.py:485} INFO - 22/07/06 07:29:42 INFO SecurityManager: Changing modify acls groups to:
[2022-07-06 07:29:42,318] {spark_submit.py:485} INFO - 22/07/06 07:29:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(default); groups with view permissions: Set(); users  with modify permissions: Set(default); groups with modify permissions: Set()
[2022-07-06 07:29:45,683] {spark_submit.py:485} INFO - 22/07/06 07:29:45 INFO Utils: Successfully started service 'sparkDriver' on port 38529.
[2022-07-06 07:29:45,878] {spark_submit.py:485} INFO - 22/07/06 07:29:45 INFO SparkEnv: Registering MapOutputTracker
[2022-07-06 07:29:46,001] {spark_submit.py:485} INFO - 22/07/06 07:29:46 INFO SparkEnv: Registering BlockManagerMaster
[2022-07-06 07:29:46,081] {spark_submit.py:485} INFO - 22/07/06 07:29:46 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-07-06 07:29:46,085] {spark_submit.py:485} INFO - 22/07/06 07:29:46 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-07-06 07:29:46,103] {spark_submit.py:485} INFO - 22/07/06 07:29:46 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-07-06 07:29:46,181] {spark_submit.py:485} INFO - 22/07/06 07:29:46 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9fe49b52-54c7-4817-b992-302b9974b4b1
[2022-07-06 07:29:46,307] {spark_submit.py:485} INFO - 22/07/06 07:29:46 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2022-07-06 07:29:46,369] {spark_submit.py:485} INFO - 22/07/06 07:29:46 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-07-06 07:29:47,461] {spark_submit.py:485} INFO - 22/07/06 07:29:47 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2022-07-06 07:29:47,689] {spark_submit.py:485} INFO - 22/07/06 07:29:47 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://538ddc85acbc:4040
[2022-07-06 07:29:48,382] {spark_submit.py:485} INFO - 22/07/06 07:29:48 WARN SparkContext: Please ensure that the number of slots available on your executors is limited by the number of cores to task cpus and not another custom resource. If cores is not the limiting resource then dynamic allocation will not work properly!
[2022-07-06 07:29:49,396] {spark_submit.py:485} INFO - 22/07/06 07:29:49 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark:7077...
[2022-07-06 07:29:49,675] {spark_submit.py:485} INFO - 22/07/06 07:29:49 INFO TransportClientFactory: Successfully created connection to spark/172.20.0.7:7077 after 120 ms (0 ms spent in bootstraps)
[2022-07-06 07:29:54,436] {spark_submit.py:485} INFO - 22/07/06 07:29:54 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20220706072954-0000
[2022-07-06 07:29:54,480] {spark_submit.py:485} INFO - 22/07/06 07:29:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35023.
[2022-07-06 07:29:54,501] {spark_submit.py:485} INFO - 22/07/06 07:29:54 INFO NettyBlockTransferService: Server created on 538ddc85acbc:35023
[2022-07-06 07:29:54,932] {spark_submit.py:485} INFO - 22/07/06 07:29:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-07-06 07:29:55,135] {spark_submit.py:485} INFO - 22/07/06 07:29:55 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220706072954-0000/0 on worker-20220706072615-172.20.0.4-34743 (172.20.0.4:34743) with 1 core(s)
[2022-07-06 07:29:55,150] {spark_submit.py:485} INFO - 22/07/06 07:29:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 538ddc85acbc, 35023, None)
[2022-07-06 07:29:55,205] {spark_submit.py:485} INFO - 22/07/06 07:29:55 INFO StandaloneSchedulerBackend: Granted executor ID app-20220706072954-0000/0 on hostPort 172.20.0.4:34743 with 1 core(s), 1024.0 MiB RAM
[2022-07-06 07:29:55,208] {spark_submit.py:485} INFO - 22/07/06 07:29:55 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220706072954-0000/1 on worker-20220706072615-172.20.0.5-44257 (172.20.0.5:44257) with 1 core(s)
[2022-07-06 07:29:55,217] {spark_submit.py:485} INFO - 22/07/06 07:29:55 INFO StandaloneSchedulerBackend: Granted executor ID app-20220706072954-0000/1 on hostPort 172.20.0.5:44257 with 1 core(s), 1024.0 MiB RAM
[2022-07-06 07:29:55,348] {spark_submit.py:485} INFO - 22/07/06 07:29:55 INFO BlockManagerMasterEndpoint: Registering block manager 538ddc85acbc:35023 with 434.4 MiB RAM, BlockManagerId(driver, 538ddc85acbc, 35023, None)
[2022-07-06 07:29:55,435] {spark_submit.py:485} INFO - 22/07/06 07:29:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 538ddc85acbc, 35023, None)
[2022-07-06 07:29:55,458] {spark_submit.py:485} INFO - 22/07/06 07:29:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 538ddc85acbc, 35023, None)
[2022-07-06 07:29:57,807] {spark_submit.py:485} INFO - 22/07/06 07:29:57 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:29:57,819] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:29:57,821] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:29:57,826] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:29:57,844] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:29:57,849] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:29:57,859] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:29:57,872] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:29:57,877] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:29:57,885] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:29:57,906] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:29:57,908] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:29:57,911] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:29:57,923] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:29:57,929] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:29:57,936] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:29:57,948] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:29:57,951] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:29:57,956] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:29:57,963] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:29:57,975] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:29:57,983] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:29:57,993] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:29:57,995] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:29:58,003] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:29:58,011] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:29:58,026] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:29:58,029] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:29:58,052] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:29:58,063] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:29:58,088] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:29:58,090] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:29:58,092] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:29:58,095] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:29:58,103] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:29:58,110] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:29:58,137] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:29:58,144] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:29:58,155] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:29:58,165] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:29:58,193] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:29:58,196] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:29:58,207] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:29:58,215] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:29:58,220] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:29:58,226] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:29:58,241] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:29:58,243] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:29:58,252] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:29:58,253] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:29:58,255] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:29:58,257] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:29:58,261] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:29:58,263] {spark_submit.py:485} INFO - 22/07/06 07:29:58 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:29:58,301] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:29:58,310] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:29:58,348] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:29:58,360] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:29:58,362] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:29:58,369] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:29:58,383] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:29:58,388] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:29:58,418] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:29:58,456] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:29:58,465] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:29:58,506] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:29:58,515] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:29:58,537] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:29:58,549] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:29:58,560] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:29:58,572] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:29:58,582] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:29:58,593] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:29:58,609] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:29:58,654] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:29:58,664] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:29:58,676] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:29:58,707] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:29:58,735] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:29:58,751] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:29:58,812] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:29:58,876] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:29:58,886] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:29:58,890] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:29:58,989] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:29:59,010] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:29:59,024] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:29:59,047] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:29:59,109] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:29:59,471] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:29:59,511] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:29:59,531] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:29:59,563] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:29:59,710] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:29:59,719] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:29:59,737] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:29:59,821] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:29:59,987] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:30:00,096] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:30:00,521] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:30:00,560] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:30:00,581] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:30:00,597] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:30:00,669] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:30:00,680] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:30:00,700] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:30:07,735] {spark_submit.py:485} INFO - 22/07/06 07:30:07 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2022-07-06 07:30:07,810] {spark_submit.py:485} INFO - 22/07/06 07:30:07 INFO AsyncEventQueue: Process of event SparkListenerBlockManagerAdded(1657092595249,BlockManagerId(driver, 538ddc85acbc, 35023, None),455501414,Some(455501414),Some(0)) by listener AppStatusListener took 1.0093415s.
[2022-07-06 07:30:23,523] {spark_submit.py:485} INFO - 22/07/06 07:30:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 175.8 KiB, free 434.2 MiB)
[2022-07-06 07:30:25,561] {spark_submit.py:485} INFO - 22/07/06 07:30:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.1 KiB, free 434.2 MiB)
[2022-07-06 07:30:25,619] {spark_submit.py:485} INFO - 22/07/06 07:30:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 538ddc85acbc:35023 (size: 27.1 KiB, free: 434.4 MiB)
[2022-07-06 07:30:25,677] {spark_submit.py:485} INFO - 22/07/06 07:30:25 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0
[2022-07-06 07:30:25,955] {spark_submit.py:485} INFO - 22/07/06 07:30:25 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-07-06 07:30:26,937] {spark_submit.py:485} INFO - 22/07/06 07:30:26 INFO FileInputFormat: Total input files to process : 1
[2022-07-06 07:30:27,605] {spark_submit.py:485} INFO - 22/07/06 07:30:27 INFO SparkContext: Starting job: count at /usr/local/spark/app/hello-world.py:23
[2022-07-06 07:30:28,373] {spark_submit.py:485} INFO - 22/07/06 07:30:28 INFO DAGScheduler: Got job 0 (count at /usr/local/spark/app/hello-world.py:23) with 2 output partitions
[2022-07-06 07:30:28,756] {spark_submit.py:485} INFO - 22/07/06 07:30:28 INFO DAGScheduler: Final stage: ResultStage 0 (count at /usr/local/spark/app/hello-world.py:23)
[2022-07-06 07:30:28,787] {spark_submit.py:485} INFO - 22/07/06 07:30:28 INFO DAGScheduler: Parents of final stage: List()
[2022-07-06 07:30:29,626] {spark_submit.py:485} INFO - 22/07/06 07:30:29 INFO DAGScheduler: Missing parents: List()
[2022-07-06 07:30:30,269] {spark_submit.py:485} INFO - 22/07/06 07:30:30 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /usr/local/spark/app/hello-world.py:23), which has no missing parents
[2022-07-06 07:30:30,940] {spark_submit.py:485} INFO - 22/07/06 07:30:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.2 KiB, free 434.2 MiB)
[2022-07-06 07:30:30,976] {spark_submit.py:485} INFO - 22/07/06 07:30:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 434.2 MiB)
[2022-07-06 07:30:30,992] {spark_submit.py:485} INFO - 22/07/06 07:30:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 538ddc85acbc:35023 (size: 4.8 KiB, free: 434.4 MiB)
[2022-07-06 07:30:31,018] {spark_submit.py:485} INFO - 22/07/06 07:30:31 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
[2022-07-06 07:30:31,182] {spark_submit.py:485} INFO - 22/07/06 07:30:31 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at count at /usr/local/spark/app/hello-world.py:23) (first 15 tasks are for partitions Vector(0, 1))
[2022-07-06 07:30:31,190] {spark_submit.py:485} INFO - 22/07/06 07:30:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
[2022-07-06 07:30:31,276] {spark_submit.py:485} INFO - 22/07/06 07:30:31 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1657092629622,WrappedArray(org.apache.spark.scheduler.StageInfo@7c098132),{spark.rdd.scope={"id":"1","name":"collect"}, callSite.short=count at /usr/local/spark/app/hello-world.py:23, spark.rdd.scope.noOverride=true}) by listener AppStatusListener took 1.5600006s.
[2022-07-06 07:30:46,463] {spark_submit.py:485} INFO - 22/07/06 07:30:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:31:01,455] {spark_submit.py:485} INFO - 22/07/06 07:31:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:31:16,455] {spark_submit.py:485} INFO - 22/07/06 07:31:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:31:31,456] {spark_submit.py:485} INFO - 22/07/06 07:31:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:31:46,458] {spark_submit.py:485} INFO - 22/07/06 07:31:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:32:01,455] {spark_submit.py:485} INFO - 22/07/06 07:32:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:32:16,455] {spark_submit.py:485} INFO - 22/07/06 07:32:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:32:26,365] {spark_submit.py:485} INFO - 22/07/06 07:32:26 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:32:26,371] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:32:26,381] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:32:26,386] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:32:26,392] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:32:26,394] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:32:26,398] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:32:26,400] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:32:26,402] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:32:26,405] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:32:26,408] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:32:26,410] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:32:26,412] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:32:26,415] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:32:26,417] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:32:26,419] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:32:26,421] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:32:26,432] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:32:26,439] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:32:26,441] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:32:26,443] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:32:26,446] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:32:26,449] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:32:26,453] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:32:26,455] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:32:26,459] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:32:26,461] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:32:26,463] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:32:26,469] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:32:26,471] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:32:26,473] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:32:26,474] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:32:26,477] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:32:26,479] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:32:26,482] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:32:26,483] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:32:26,486] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:32:26,488] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:32:26,491] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:32:26,495] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:32:26,498] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:32:26,501] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:32:26,506] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:32:26,508] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:32:26,511] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:32:26,513] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:32:26,519] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:32:26,522] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:32:26,527] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:32:26,530] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:32:26,533] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:32:26,547] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:32:26,556] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:32:26,561] {spark_submit.py:485} INFO - 22/07/06 07:32:26 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220706072954-0000/2 on worker-20220706072615-172.20.0.5-44257 (172.20.0.5:44257) with 1 core(s)
[2022-07-06 07:32:26,563] {spark_submit.py:485} INFO - 22/07/06 07:32:26 INFO StandaloneSchedulerBackend: Granted executor ID app-20220706072954-0000/2 on hostPort 172.20.0.5:44257 with 1 core(s), 1024.0 MiB RAM
[2022-07-06 07:32:26,595] {spark_submit.py:485} INFO - 22/07/06 07:32:26 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:32:26,597] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:32:26,600] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:32:26,602] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:32:26,605] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:32:26,608] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:32:26,610] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:32:26,612] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:32:26,620] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:32:26,624] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:32:26,627] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:32:26,629] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:32:26,631] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:32:26,633] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:32:26,636] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:32:26,639] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:32:26,640] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:32:26,642] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:32:26,644] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:32:26,653] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:32:26,655] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:32:26,672] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:32:26,674] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:32:26,680] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:32:26,686] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:32:26,694] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:32:26,701] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:32:26,703] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:32:26,709] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:32:26,719] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:32:26,726] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:32:26,734] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:32:26,735] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:32:26,739] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:32:26,748] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:32:26,750] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:32:26,758] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:32:26,767] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:32:26,770] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:32:26,771] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:32:26,773] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:32:26,775] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:32:26,778] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:32:26,784] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:32:26,786] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:32:26,788] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:32:26,792] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:32:26,794] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:32:26,796] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:32:26,798] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:32:26,807] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:32:26,809] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:32:26,816] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:32:26,818] {spark_submit.py:485} INFO - 22/07/06 07:32:26 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220706072954-0000/3 on worker-20220706072615-172.20.0.4-34743 (172.20.0.4:34743) with 1 core(s)
[2022-07-06 07:32:26,821] {spark_submit.py:485} INFO - 22/07/06 07:32:26 INFO StandaloneSchedulerBackend: Granted executor ID app-20220706072954-0000/3 on hostPort 172.20.0.4:34743 with 1 core(s), 1024.0 MiB RAM
[2022-07-06 07:32:26,887] {spark_submit.py:485} INFO - 22/07/06 07:32:26 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:32:26,891] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:32:26,893] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:32:26,894] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:32:26,896] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:32:26,897] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:32:26,899] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:32:26,900] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:32:26,902] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:32:26,904] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:32:26,907] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:32:26,908] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:32:26,910] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:32:26,911] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:32:26,920] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:32:26,922] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:32:26,924] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:32:26,926] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:32:26,927] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:32:26,929] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:32:26,931] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:32:26,933] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:32:26,935] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:32:26,950] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:32:26,952] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:32:26,954] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:32:26,957] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:32:26,959] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:32:26,961] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:32:26,963] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:32:26,965] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:32:26,967] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:32:26,969] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:32:26,970] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:32:26,972] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:32:26,975] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:32:26,977] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:32:26,981] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:32:26,983] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:32:26,985] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:32:26,986] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:32:26,988] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:32:26,990] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:32:26,993] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:32:26,996] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:32:26,998] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:32:27,000] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:32:27,003] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:32:27,006] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:32:27,007] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:32:27,009] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:32:27,011] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:32:27,012] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:32:27,723] {spark_submit.py:485} INFO - 22/07/06 07:32:27 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:32:27,725] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:32:27,728] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:32:27,735] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:32:27,738] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:32:27,745] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:32:27,749] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:32:27,752] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:32:27,756] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:32:27,761] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:32:27,765] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:32:27,771] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:32:27,777] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:32:27,788] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:32:27,795] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:32:27,798] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:32:27,801] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:32:27,803] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:32:27,805] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:32:27,809] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:32:27,811] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:32:27,814] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:32:27,816] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:32:27,818] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:32:27,820] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:32:27,822] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:32:27,825] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:32:27,828] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:32:27,830] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:32:27,833] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:32:27,835] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:32:27,838] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:32:27,842] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:32:27,845] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:32:27,847] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:32:27,850] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:32:27,852] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:32:27,854] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:32:27,859] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:32:27,861] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:32:27,864] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:32:27,869] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:32:27,875] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:32:27,879] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:32:27,883] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:32:27,889] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:32:27,901] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:32:27,905] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:32:27,909] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:32:27,914] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:32:27,917] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:32:27,923] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:32:27,940] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:32:31,454] {spark_submit.py:485} INFO - 22/07/06 07:32:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:32:46,457] {spark_submit.py:485} INFO - 22/07/06 07:32:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:33:01,454] {spark_submit.py:485} INFO - 22/07/06 07:33:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:33:16,456] {spark_submit.py:485} INFO - 22/07/06 07:33:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:33:31,454] {spark_submit.py:485} INFO - 22/07/06 07:33:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:33:46,454] {spark_submit.py:485} INFO - 22/07/06 07:33:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:34:01,455] {spark_submit.py:485} INFO - 22/07/06 07:34:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:34:16,457] {spark_submit.py:485} INFO - 22/07/06 07:34:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:34:31,457] {spark_submit.py:485} INFO - 22/07/06 07:34:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:34:44,050] {spark_submit.py:485} INFO - 22/07/06 07:34:44 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:34:44,053] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:34:44,055] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:34:44,057] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:34:44,059] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:34:44,062] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:34:44,070] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:34:44,072] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:34:44,074] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:34:44,107] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:34:44,111] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:34:44,117] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:34:44,167] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:34:44,173] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:34:44,187] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:34:44,189] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:34:44,195] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:34:44,203] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:34:44,206] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:34:44,209] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:34:44,211] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:34:44,215] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:34:44,218] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:34:44,222] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:34:44,225] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:34:44,236] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:34:44,241] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:34:44,243] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:34:44,247] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:34:44,249] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:34:44,253] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:34:44,254] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:34:44,256] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:34:44,257] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:34:44,259] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:34:44,260] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:34:44,268] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:34:44,279] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:34:44,280] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:34:44,284] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:34:44,285] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:34:44,294] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:34:44,303] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:34:44,314] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:34:44,325] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:34:44,338] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:34:44,346] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:34:44,357] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:34:44,361] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:34:44,363] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:34:44,366] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:34:44,367] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:34:44,370] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:34:44,373] {spark_submit.py:485} INFO - 22/07/06 07:34:44 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220706072954-0000/4 on worker-20220706072615-172.20.0.4-34743 (172.20.0.4:34743) with 1 core(s)
[2022-07-06 07:34:44,376] {spark_submit.py:485} INFO - 22/07/06 07:34:44 INFO StandaloneSchedulerBackend: Granted executor ID app-20220706072954-0000/4 on hostPort 172.20.0.4:34743 with 1 core(s), 1024.0 MiB RAM
[2022-07-06 07:34:44,378] {spark_submit.py:485} INFO - 22/07/06 07:34:44 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:34:44,380] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:34:44,383] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:34:44,390] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:34:44,392] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:34:44,395] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:34:44,397] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:34:44,401] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:34:44,406] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:34:44,408] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:34:44,414] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:34:44,416] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:34:44,419] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:34:44,422] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:34:44,428] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:34:44,430] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:34:44,434] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:34:44,436] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:34:44,438] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:34:44,453] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:34:44,461] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:34:44,463] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:34:44,466] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:34:44,468] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:34:44,470] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:34:44,475] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:34:44,481] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:34:44,485] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:34:44,488] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:34:44,491] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:34:44,497] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:34:44,499] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:34:44,506] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:34:44,508] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:34:44,516] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:34:44,519] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:34:44,522] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:34:44,531] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:34:44,533] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:34:44,535] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:34:44,542] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:34:44,548] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:34:44,552] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:34:44,554] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:34:44,557] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:34:44,568] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:34:44,570] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:34:44,582] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:34:44,584] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:34:44,586] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:34:44,589] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:34:44,590] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:34:44,592] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:34:44,598] {spark_submit.py:485} INFO - 22/07/06 07:34:44 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220706072954-0000/5 on worker-20220706072615-172.20.0.5-44257 (172.20.0.5:44257) with 1 core(s)
[2022-07-06 07:34:44,599] {spark_submit.py:485} INFO - 22/07/06 07:34:44 INFO StandaloneSchedulerBackend: Granted executor ID app-20220706072954-0000/5 on hostPort 172.20.0.5:44257 with 1 core(s), 1024.0 MiB RAM
[2022-07-06 07:34:44,601] {spark_submit.py:485} INFO - 22/07/06 07:34:44 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:34:44,604] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:34:44,606] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:34:44,608] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:34:44,609] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:34:44,614] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:34:44,616] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:34:44,617] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:34:44,619] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:34:44,621] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:34:44,623] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:34:44,625] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:34:44,634] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:34:44,636] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:34:44,638] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:34:44,640] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:34:44,641] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:34:44,643] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:34:44,648] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:34:44,650] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:34:44,651] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:34:44,654] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:34:44,658] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:34:44,676] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:34:44,679] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:34:44,682] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:34:44,684] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:34:44,685] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:34:44,687] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:34:44,690] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:34:44,691] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:34:44,693] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:34:44,697] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:34:44,703] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:34:44,704] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:34:44,706] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:34:44,708] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:34:44,709] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:34:44,720] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:34:44,722] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:34:44,727] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:34:44,731] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:34:44,733] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:34:44,735] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:34:44,736] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:34:44,738] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:34:44,740] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:34:44,742] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:34:44,743] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:34:44,752] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:34:44,755] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:34:44,758] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:34:44,760] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:34:44,774] {spark_submit.py:485} INFO - 22/07/06 07:34:44 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:34:44,778] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:34:44,783] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:34:44,789] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:34:44,814] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:34:44,820] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:34:44,828] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:34:44,832] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:34:44,834] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:34:44,837] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:34:44,844] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:34:44,846] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:34:44,856] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:34:44,859] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:34:44,862] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:34:44,873] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:34:44,878] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:34:44,880] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:34:44,884] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:34:44,888] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:34:44,896] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:34:44,899] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:34:44,906] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:34:44,908] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:34:44,910] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:34:44,913] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:34:44,919] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:34:44,922] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:34:44,932] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:34:44,934] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:34:44,942] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:34:44,949] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:34:44,952] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:34:44,958] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:34:44,962] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:34:44,967] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:34:44,970] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:34:44,971] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:34:44,973] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:34:44,984] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:34:44,986] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:34:44,989] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:34:44,991] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:34:44,993] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:34:44,996] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:34:44,998] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:34:45,000] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:34:45,003] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:34:45,005] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:34:45,008] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:34:45,010] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:34:45,012] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:34:45,019] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:34:46,454] {spark_submit.py:485} INFO - 22/07/06 07:34:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:35:01,456] {spark_submit.py:485} INFO - 22/07/06 07:35:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:35:16,456] {spark_submit.py:485} INFO - 22/07/06 07:35:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:35:31,458] {spark_submit.py:485} INFO - 22/07/06 07:35:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:35:46,456] {spark_submit.py:485} INFO - 22/07/06 07:35:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:36:01,457] {spark_submit.py:485} INFO - 22/07/06 07:36:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:36:16,456] {spark_submit.py:485} INFO - 22/07/06 07:36:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:36:31,456] {spark_submit.py:485} INFO - 22/07/06 07:36:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:36:46,457] {spark_submit.py:485} INFO - 22/07/06 07:36:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:36:58,141] {spark_submit.py:485} INFO - 22/07/06 07:36:58 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:36:58,143] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:36:58,148] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:36:58,151] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:36:58,156] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:36:58,160] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:36:58,171] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:36:58,177] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:36:58,180] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:36:58,182] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:36:58,186] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:36:58,188] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:36:58,189] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:36:58,191] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:36:58,193] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:36:58,194] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:36:58,197] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:36:58,199] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:36:58,200] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:36:58,202] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:36:58,204] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:36:58,206] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:36:58,208] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:36:58,213] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:36:58,215] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:36:58,216] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:36:58,218] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:36:58,220] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:36:58,222] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:36:58,228] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:36:58,233] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:36:58,235] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:36:58,237] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:36:58,239] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:36:58,240] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:36:58,241] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:36:58,243] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:36:58,247] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:36:58,249] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:36:58,252] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:36:58,255] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:36:58,262] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:36:58,266] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:36:58,269] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:36:58,272] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:36:58,274] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:36:58,276] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:36:58,278] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:36:58,281] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:36:58,291] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:36:58,294] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:36:58,304] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:36:58,307] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:36:58,313] {spark_submit.py:485} INFO - 22/07/06 07:36:58 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220706072954-0000/6 on worker-20220706072615-172.20.0.4-34743 (172.20.0.4:34743) with 1 core(s)
[2022-07-06 07:36:58,315] {spark_submit.py:485} INFO - 22/07/06 07:36:58 INFO StandaloneSchedulerBackend: Granted executor ID app-20220706072954-0000/6 on hostPort 172.20.0.4:34743 with 1 core(s), 1024.0 MiB RAM
[2022-07-06 07:36:58,392] {spark_submit.py:485} INFO - 22/07/06 07:36:58 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:36:58,394] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:36:58,396] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:36:58,398] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:36:58,400] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:36:58,403] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:36:58,405] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:36:58,407] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:36:58,409] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:36:58,434] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:36:58,442] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:36:58,444] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:36:58,464] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:36:58,467] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:36:58,502] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:36:58,504] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:36:58,529] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:36:58,531] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:36:58,534] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:36:58,545] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:36:58,549] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:36:58,552] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:36:58,554] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:36:58,557] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:36:58,560] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:36:58,563] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:36:58,565] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:36:58,567] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:36:58,569] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:36:58,572] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:36:58,573] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:36:58,576] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:36:58,578] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:36:58,580] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:36:58,582] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:36:58,586] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:36:58,589] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:36:58,592] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:36:58,595] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:36:58,599] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:36:58,604] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:36:58,608] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:36:58,610] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:36:58,613] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:36:58,615] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:36:58,618] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:36:58,620] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:36:58,623] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:36:58,627] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:36:58,629] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:36:58,631] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:36:58,634] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:36:58,637] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:36:58,640] {spark_submit.py:485} INFO - 22/07/06 07:36:58 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:36:58,643] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:36:58,645] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:36:58,647] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:36:58,649] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:36:58,651] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:36:58,653] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:36:58,656] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:36:58,658] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:36:58,660] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:36:58,662] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:36:58,665] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:36:58,667] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:36:58,669] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:36:58,671] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:36:58,674] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:36:58,677] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:36:58,681] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:36:58,683] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:36:58,684] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:36:58,687] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:36:58,689] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:36:58,692] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:36:58,694] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:36:58,697] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:36:58,701] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:36:58,707] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:36:58,712] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:36:58,715] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:36:58,717] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:36:58,720] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:36:58,722] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:36:58,726] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:36:58,729] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:36:58,731] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:36:58,733] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:36:58,735] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:36:58,738] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:36:58,741] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:36:58,743] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:36:58,746] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:36:58,750] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:36:58,753] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:36:58,756] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:36:58,758] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:36:58,760] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:36:58,762] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:36:58,764] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:36:58,767] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:36:58,769] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:36:58,771] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:36:58,773] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:36:58,775] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:36:58,777] {spark_submit.py:485} INFO - 22/07/06 07:36:58 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220706072954-0000/7 on worker-20220706072615-172.20.0.5-44257 (172.20.0.5:44257) with 1 core(s)
[2022-07-06 07:36:58,780] {spark_submit.py:485} INFO - 22/07/06 07:36:58 INFO StandaloneSchedulerBackend: Granted executor ID app-20220706072954-0000/7 on hostPort 172.20.0.5:44257 with 1 core(s), 1024.0 MiB RAM
[2022-07-06 07:36:58,937] {spark_submit.py:485} INFO - 22/07/06 07:36:58 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:36:58,940] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:36:58,944] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:36:58,947] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:36:58,950] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:36:58,952] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:36:58,955] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:36:58,957] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:36:58,960] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:36:58,963] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:36:58,965] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:36:58,969] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:36:58,972] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:36:58,978] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:36:58,983] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:36:58,985] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:36:58,988] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:36:58,991] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:36:58,995] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:36:58,998] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:36:59,000] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:36:59,003] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:36:59,006] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:36:59,010] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:36:59,013] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:36:59,016] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:36:59,019] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:36:59,023] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:36:59,025] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:36:59,027] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:36:59,030] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:36:59,032] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:36:59,037] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:36:59,039] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:36:59,042] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:36:59,044] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:36:59,047] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:36:59,050] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:36:59,052] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:36:59,054] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:36:59,059] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:36:59,062] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:36:59,064] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:36:59,066] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:36:59,072] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:36:59,075] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:36:59,077] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:36:59,079] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:36:59,081] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:36:59,084] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:36:59,086] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:36:59,088] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:36:59,091] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:37:01,454] {spark_submit.py:485} INFO - 22/07/06 07:37:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:37:16,463] {spark_submit.py:485} INFO - 22/07/06 07:37:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:37:31,458] {spark_submit.py:485} INFO - 22/07/06 07:37:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:37:46,454] {spark_submit.py:485} INFO - 22/07/06 07:37:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:38:01,458] {spark_submit.py:485} INFO - 22/07/06 07:38:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:38:16,455] {spark_submit.py:485} INFO - 22/07/06 07:38:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:38:31,455] {spark_submit.py:485} INFO - 22/07/06 07:38:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:38:46,459] {spark_submit.py:485} INFO - 22/07/06 07:38:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:39:01,455] {spark_submit.py:485} INFO - 22/07/06 07:39:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:39:06,757] {spark_submit.py:485} INFO - 22/07/06 07:39:06 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:39:06,760] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:39:06,767] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:39:06,769] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:39:06,770] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:39:06,772] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:39:06,773] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:39:06,775] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:39:06,777] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:39:06,778] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:39:06,781] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:39:06,783] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:39:06,786] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:39:06,788] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:39:06,790] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:39:06,791] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:39:06,795] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:39:06,797] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:39:06,799] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:39:06,802] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:39:06,805] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:39:06,806] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:39:06,808] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:39:06,810] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:39:06,812] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:39:06,813] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:39:06,815] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:39:06,817] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:39:06,819] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:39:06,829] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:39:06,831] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:39:06,833] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:39:06,835] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:39:06,837] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:39:06,839] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:39:06,841] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:39:06,843] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:39:06,845] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:39:06,847] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:39:06,849] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:39:06,852] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:39:06,854] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:39:06,856] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:39:06,859] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:39:06,862] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:39:06,866] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:39:06,873] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:39:06,875] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:39:06,878] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:39:06,880] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:39:06,882] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:39:06,884] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:39:06,887] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:39:06,889] {spark_submit.py:485} INFO - 22/07/06 07:39:06 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220706072954-0000/8 on worker-20220706072615-172.20.0.4-34743 (172.20.0.4:34743) with 1 core(s)
[2022-07-06 07:39:06,892] {spark_submit.py:485} INFO - 22/07/06 07:39:06 INFO StandaloneSchedulerBackend: Granted executor ID app-20220706072954-0000/8 on hostPort 172.20.0.4:34743 with 1 core(s), 1024.0 MiB RAM
[2022-07-06 07:39:07,143] {spark_submit.py:485} INFO - 22/07/06 07:39:07 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:39:07,146] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:39:07,149] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:39:07,150] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:39:07,160] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:39:07,163] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:39:07,167] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:39:07,184] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:39:07,186] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:39:07,187] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:39:07,190] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:39:07,194] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:39:07,209] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:39:07,210] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:39:07,217] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:39:07,219] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:39:07,221] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:39:07,225] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:39:07,228] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:39:07,232] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:39:07,237] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:39:07,240] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:39:07,242] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:39:07,245] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:39:07,247] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:39:07,249] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:39:07,251] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:39:07,253] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:39:07,257] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:39:07,259] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:39:07,263] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:39:07,264] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:39:07,270] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:39:07,272] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:39:07,274] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:39:07,276] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:39:07,279] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:39:07,296] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:39:07,324] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:39:07,338] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:39:07,355] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:39:07,375] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:39:07,394] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:39:07,425] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:39:07,435] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:39:07,437] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:39:07,440] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:39:07,444] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:39:07,449] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:39:07,453] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:39:07,455] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:39:07,480] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:39:07,502] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:39:07,507] {spark_submit.py:485} INFO - 22/07/06 07:39:07 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220706072954-0000/9 on worker-20220706072615-172.20.0.5-44257 (172.20.0.5:44257) with 1 core(s)
[2022-07-06 07:39:07,517] {spark_submit.py:485} INFO - 22/07/06 07:39:07 INFO StandaloneSchedulerBackend: Granted executor ID app-20220706072954-0000/9 on hostPort 172.20.0.5:44257 with 1 core(s), 1024.0 MiB RAM
[2022-07-06 07:39:07,676] {spark_submit.py:485} INFO - 22/07/06 07:39:07 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:39:07,678] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:39:07,680] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:39:07,682] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:39:07,686] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:39:07,690] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:39:07,695] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:39:07,703] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:39:07,712] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:39:07,714] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:39:07,716] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:39:07,718] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:39:07,799] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:39:07,801] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:39:07,808] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:39:07,810] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:39:07,812] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:39:07,815] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:39:07,817] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:39:07,820] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:39:07,821] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:39:07,823] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:39:07,825] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:39:07,829] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:39:07,832] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:39:07,834] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:39:07,837] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:39:07,842] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:39:07,844] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:39:07,845] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:39:07,847] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:39:07,849] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:39:07,851] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:39:07,852] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:39:07,854] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:39:07,856] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:39:07,858] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:39:07,859] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:39:07,861] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:39:07,862] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:39:07,864] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:39:07,866] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:39:07,868] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:39:07,869] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:39:07,872] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:39:07,874] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:39:07,876] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:39:07,878] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:39:07,879] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:39:07,881] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:39:07,886] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:39:07,887] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:39:07,889] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:39:07,954] {spark_submit.py:485} INFO - 22/07/06 07:39:07 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:39:07,982] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:39:07,994] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:39:08,009] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:39:08,018] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:39:08,069] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:39:08,077] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:39:08,135] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:39:08,149] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:39:08,157] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:39:08,159] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:39:08,162] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:39:08,166] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:39:08,174] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:39:08,177] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:39:08,182] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:39:08,183] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:39:08,186] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:39:08,189] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:39:08,190] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:39:08,193] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:39:08,195] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:39:08,198] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:39:08,200] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:39:08,218] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:39:08,222] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:39:08,224] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:39:08,226] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:39:08,245] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:39:08,257] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:39:08,261] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:39:08,292] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:39:08,313] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:39:08,315] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:39:08,318] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:39:08,319] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:39:08,322] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:39:08,324] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:39:08,326] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:39:08,328] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:39:08,343] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:39:08,410] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:39:08,428] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:39:08,441] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:39:08,442] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:39:08,444] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:39:08,446] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:39:08,448] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:39:08,450] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:39:08,452] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:39:08,453] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:39:08,469] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:39:08,474] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:39:16,457] {spark_submit.py:485} INFO - 22/07/06 07:39:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:39:31,459] {spark_submit.py:485} INFO - 22/07/06 07:39:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:39:46,455] {spark_submit.py:485} INFO - 22/07/06 07:39:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:40:01,455] {spark_submit.py:485} INFO - 22/07/06 07:40:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:40:16,456] {spark_submit.py:485} INFO - 22/07/06 07:40:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:40:31,454] {spark_submit.py:485} INFO - 22/07/06 07:40:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:40:46,459] {spark_submit.py:485} INFO - 22/07/06 07:40:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:41:01,454] {spark_submit.py:485} INFO - 22/07/06 07:41:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:41:16,454] {spark_submit.py:485} INFO - 22/07/06 07:41:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:41:19,726] {spark_submit.py:485} INFO - 22/07/06 07:41:19 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:41:19,729] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:41:19,732] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:41:19,735] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:41:19,739] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:41:19,741] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:41:19,744] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:41:19,747] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:41:19,753] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:41:19,756] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:41:19,759] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:41:19,763] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:41:19,766] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:41:19,769] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:41:19,772] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:41:19,775] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:41:19,778] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:41:19,781] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:41:19,784] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:41:19,786] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:41:19,789] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:41:19,790] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:41:19,793] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:41:19,796] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:41:19,799] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:41:19,801] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:41:19,805] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:41:19,808] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:41:19,810] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:41:19,812] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:41:19,815] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:41:19,821] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:41:19,824] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:41:19,828] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:41:19,832] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:41:19,834] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:41:19,835] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:41:19,840] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:41:19,842] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:41:19,846] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:41:19,849] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:41:19,855] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:41:19,858] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:41:19,862] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:41:19,864] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:41:19,867] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:41:19,869] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:41:19,871] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:41:19,873] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:41:19,875] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:41:19,877] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:41:19,879] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:41:19,881] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:41:19,884] {spark_submit.py:485} INFO - 22/07/06 07:41:19 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220706072954-0000/10 on worker-20220706072615-172.20.0.4-34743 (172.20.0.4:34743) with 1 core(s)
[2022-07-06 07:41:19,886] {spark_submit.py:485} INFO - 22/07/06 07:41:19 INFO StandaloneSchedulerBackend: Granted executor ID app-20220706072954-0000/10 on hostPort 172.20.0.4:34743 with 1 core(s), 1024.0 MiB RAM
[2022-07-06 07:41:19,913] {spark_submit.py:485} INFO - 22/07/06 07:41:19 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:41:19,914] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:41:19,917] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:41:19,919] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:41:19,921] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:41:19,941] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:41:19,944] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:41:19,947] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:41:19,966] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:41:19,968] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:41:19,972] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:41:19,976] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:41:19,978] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:41:19,983] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:41:19,988] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:41:20,006] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:41:20,008] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:41:20,010] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:41:20,023] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:41:20,024] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:41:20,026] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:41:20,028] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:41:20,029] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:41:20,031] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:41:20,033] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:41:20,034] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:41:20,036] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:41:20,038] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:41:20,040] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:41:20,042] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:41:20,044] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:41:20,046] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:41:20,048] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:41:20,050] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:41:20,052] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:41:20,054] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:41:20,056] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:41:20,063] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:41:20,068] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:41:20,075] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:41:20,076] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:41:20,079] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:41:20,081] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:41:20,082] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:41:20,084] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:41:20,086] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:41:20,087] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:41:20,089] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:41:20,090] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:41:20,091] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:41:20,094] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:41:20,096] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:41:20,097] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:41:20,110] {spark_submit.py:485} INFO - 22/07/06 07:41:20 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:41:20,111] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:41:20,113] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:41:20,117] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:41:20,119] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:41:20,121] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:41:20,135] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:41:20,147] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:41:20,149] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:41:20,152] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:41:20,153] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:41:20,155] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:41:20,157] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:41:20,159] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:41:20,161] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:41:20,163] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:41:20,165] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:41:20,168] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:41:20,170] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:41:20,172] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:41:20,178] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:41:20,180] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:41:20,182] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:41:20,187] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:41:20,189] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:41:20,191] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:41:20,192] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:41:20,194] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:41:20,196] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:41:20,197] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:41:20,199] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:41:20,203] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:41:20,206] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:41:20,209] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:41:20,211] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:41:20,213] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:41:20,217] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:41:20,218] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:41:20,220] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:41:20,221] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:41:20,223] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:41:20,224] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:41:20,226] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:41:20,229] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:41:20,231] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:41:20,232] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:41:20,234] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:41:20,236] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:41:20,237] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:41:20,239] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:41:20,241] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:41:20,243] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:41:20,245] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:41:20,247] {spark_submit.py:485} INFO - 22/07/06 07:41:20 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220706072954-0000/11 on worker-20220706072615-172.20.0.5-44257 (172.20.0.5:44257) with 1 core(s)
[2022-07-06 07:41:20,249] {spark_submit.py:485} INFO - 22/07/06 07:41:20 INFO StandaloneSchedulerBackend: Granted executor ID app-20220706072954-0000/11 on hostPort 172.20.0.5:44257 with 1 core(s), 1024.0 MiB RAM
[2022-07-06 07:41:20,434] {spark_submit.py:485} INFO - 22/07/06 07:41:20 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:41:20,436] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:41:20,438] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:41:20,440] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:41:20,441] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:41:20,443] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:41:20,445] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:41:20,446] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:41:20,448] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:41:20,450] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:41:20,452] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:41:20,454] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:41:20,455] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:41:20,457] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:41:20,462] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:41:20,465] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:41:20,466] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:41:20,468] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:41:20,470] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:41:20,474] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:41:20,475] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:41:20,477] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:41:20,479] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:41:20,483] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:41:20,485] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:41:20,487] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:41:20,489] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:41:20,491] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:41:20,493] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:41:20,495] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:41:20,497] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:41:20,500] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:41:20,503] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:41:20,505] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:41:20,507] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:41:20,514] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:41:20,516] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:41:20,518] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:41:20,520] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:41:20,522] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:41:20,524] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:41:20,526] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:41:20,529] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:41:20,531] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:41:20,534] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:41:20,536] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:41:20,538] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:41:20,540] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:41:20,541] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:41:20,543] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:41:20,545] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:41:20,547] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:41:20,548] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:41:31,454] {spark_submit.py:485} INFO - 22/07/06 07:41:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:41:46,455] {spark_submit.py:485} INFO - 22/07/06 07:41:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:42:01,454] {spark_submit.py:485} INFO - 22/07/06 07:42:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:42:16,454] {spark_submit.py:485} INFO - 22/07/06 07:42:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:42:31,455] {spark_submit.py:485} INFO - 22/07/06 07:42:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:42:46,457] {spark_submit.py:485} INFO - 22/07/06 07:42:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:43:01,454] {spark_submit.py:485} INFO - 22/07/06 07:43:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:43:16,457] {spark_submit.py:485} INFO - 22/07/06 07:43:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:43:27,392] {spark_submit.py:485} INFO - 22/07/06 07:43:27 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:43:27,394] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:43:27,396] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:43:27,398] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:43:27,402] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:43:27,404] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:43:27,410] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:43:27,412] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:43:27,413] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:43:27,418] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:43:27,419] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:43:27,421] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:43:27,424] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:43:27,427] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:43:27,430] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:43:27,431] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:43:27,432] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:43:27,434] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:43:27,436] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:43:27,438] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:43:27,439] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:43:27,441] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:43:27,445] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:43:27,447] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:43:27,448] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:43:27,450] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:43:27,451] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:43:27,453] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:43:27,455] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:43:27,456] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:43:27,458] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:43:27,460] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:43:27,462] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:43:27,464] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:43:27,465] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:43:27,467] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:43:27,469] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:43:27,470] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:43:27,472] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:43:27,473] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:43:27,474] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:43:27,476] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:43:27,477] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:43:27,479] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:43:27,482] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:43:27,484] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:43:27,485] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:43:27,487] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:43:27,488] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:43:27,490] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:43:27,491] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:43:27,492] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:43:27,493] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:43:27,495] {spark_submit.py:485} INFO - 22/07/06 07:43:27 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220706072954-0000/12 on worker-20220706072615-172.20.0.4-34743 (172.20.0.4:34743) with 1 core(s)
[2022-07-06 07:43:27,496] {spark_submit.py:485} INFO - 22/07/06 07:43:27 INFO StandaloneSchedulerBackend: Granted executor ID app-20220706072954-0000/12 on hostPort 172.20.0.4:34743 with 1 core(s), 1024.0 MiB RAM
[2022-07-06 07:43:27,563] {spark_submit.py:485} INFO - 22/07/06 07:43:27 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:43:27,565] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:43:27,567] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:43:27,570] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:43:27,573] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:43:27,575] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:43:27,577] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:43:27,579] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:43:27,581] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:43:27,583] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:43:27,585] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:43:27,587] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:43:27,589] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:43:27,591] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:43:27,593] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:43:27,595] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:43:27,597] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:43:27,599] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:43:27,600] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:43:27,602] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:43:27,606] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:43:27,610] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:43:27,612] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:43:27,614] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:43:27,616] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:43:27,620] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:43:27,623] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:43:27,625] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:43:27,626] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:43:27,628] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:43:27,632] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:43:27,635] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:43:27,637] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:43:27,639] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:43:27,641] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:43:27,643] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:43:27,644] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:43:27,646] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:43:27,648] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:43:27,649] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:43:27,651] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:43:27,653] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:43:27,654] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:43:27,655] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:43:27,657] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:43:27,659] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:43:27,661] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:43:27,670] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:43:27,671] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:43:27,673] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:43:27,675] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:43:27,676] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:43:27,678] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:43:27,725] {spark_submit.py:485} INFO - 22/07/06 07:43:27 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:43:27,727] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:43:27,730] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:43:27,732] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:43:27,734] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:43:27,737] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:43:27,739] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:43:27,743] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:43:27,745] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:43:27,747] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:43:27,748] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:43:27,750] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:43:27,752] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:43:27,754] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:43:27,756] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:43:27,758] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:43:27,759] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:43:27,761] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:43:27,764] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:43:27,767] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:43:27,779] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:43:27,781] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:43:27,788] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:43:27,791] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:43:27,794] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:43:27,797] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:43:27,802] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:43:27,807] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:43:27,810] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:43:27,812] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:43:27,813] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:43:27,816] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:43:27,818] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:43:27,824] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:43:27,826] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:43:27,828] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:43:27,830] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:43:27,832] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:43:27,833] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:43:27,836] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:43:27,839] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:43:27,841] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:43:27,843] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:43:27,845] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:43:27,849] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:43:27,851] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:43:27,855] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:43:27,860] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:43:27,865] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:43:27,868] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:43:27,872] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:43:27,874] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:43:27,876] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:43:27,879] {spark_submit.py:485} INFO - 22/07/06 07:43:27 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220706072954-0000/13 on worker-20220706072615-172.20.0.5-44257 (172.20.0.5:44257) with 1 core(s)
[2022-07-06 07:43:27,881] {spark_submit.py:485} INFO - 22/07/06 07:43:27 INFO StandaloneSchedulerBackend: Granted executor ID app-20220706072954-0000/13 on hostPort 172.20.0.5:44257 with 1 core(s), 1024.0 MiB RAM
[2022-07-06 07:43:27,987] {spark_submit.py:485} INFO - 22/07/06 07:43:27 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:43:27,989] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:43:27,992] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:43:27,999] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:43:28,002] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:43:28,004] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:43:28,007] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:43:28,009] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:43:28,011] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:43:28,013] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:43:28,015] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:43:28,017] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:43:28,019] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:43:28,020] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:43:28,022] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:43:28,024] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:43:28,026] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:43:28,028] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:43:28,029] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:43:28,030] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:43:28,032] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:43:28,033] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:43:28,035] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:43:28,037] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:43:28,038] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:43:28,040] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:43:28,041] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:43:28,043] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:43:28,044] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:43:28,045] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:43:28,050] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:43:28,052] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:43:28,057] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:43:28,059] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:43:28,061] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:43:28,063] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:43:28,065] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:43:28,067] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:43:28,069] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:43:28,076] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:43:28,079] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:43:28,081] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:43:28,082] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:43:28,084] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:43:28,086] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:43:28,088] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:43:28,090] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:43:28,093] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:43:28,095] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:43:28,098] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:43:28,101] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:43:28,104] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:43:28,106] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:43:31,454] {spark_submit.py:485} INFO - 22/07/06 07:43:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:43:46,458] {spark_submit.py:485} INFO - 22/07/06 07:43:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:44:01,454] {spark_submit.py:485} INFO - 22/07/06 07:44:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:44:16,457] {spark_submit.py:485} INFO - 22/07/06 07:44:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:44:31,454] {spark_submit.py:485} INFO - 22/07/06 07:44:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:44:46,457] {spark_submit.py:485} INFO - 22/07/06 07:44:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:45:01,455] {spark_submit.py:485} INFO - 22/07/06 07:45:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:45:16,466] {spark_submit.py:485} INFO - 22/07/06 07:45:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:45:31,455] {spark_submit.py:485} INFO - 22/07/06 07:45:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:45:37,493] {spark_submit.py:485} INFO - 22/07/06 07:45:37 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:45:37,522] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:45:37,526] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:45:37,923] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:45:37,929] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:45:37,984] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:45:38,003] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:45:38,006] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:45:38,037] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:45:38,039] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:45:38,122] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:45:38,151] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:45:38,159] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:45:38,177] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:45:38,188] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:45:38,202] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:45:38,206] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:45:38,216] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:45:38,226] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:45:38,230] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:45:38,236] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:45:38,240] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:45:38,245] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:45:38,248] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:45:38,254] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:45:38,260] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:45:38,262] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:45:38,265] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:45:38,270] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:45:38,273] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:45:38,279] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:45:38,282] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:45:38,284] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:45:38,287] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:45:38,289] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:45:38,292] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:45:38,294] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:45:38,296] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:45:38,306] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:45:38,334] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:45:38,352] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:45:38,376] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:45:38,408] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:45:38,424] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:45:38,442] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:45:38,447] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:45:38,455] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:45:38,457] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:45:38,459] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:45:38,460] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:45:38,463] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:45:38,465] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:45:38,467] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:45:38,469] {spark_submit.py:485} INFO - 22/07/06 07:45:37 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:45:38,471] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:45:38,475] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:45:38,479] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:45:38,482] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:45:38,487] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:45:38,489] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:45:38,492] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:45:38,494] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:45:38,495] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:45:38,497] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:45:38,499] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:45:38,501] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:45:38,503] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:45:38,504] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:45:38,507] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:45:38,509] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:45:38,511] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:45:38,517] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:45:38,520] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:45:38,523] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:45:38,525] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:45:38,528] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:45:38,530] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:45:38,532] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:45:38,533] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:45:38,538] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:45:38,540] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:45:38,542] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:45:38,543] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:45:38,545] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:45:38,550] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:45:38,552] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:45:38,555] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:45:38,556] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:45:38,558] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:45:38,561] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:45:38,563] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:45:38,565] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:45:38,567] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:45:38,568] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:45:38,570] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:45:38,581] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:45:38,584] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:45:38,587] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:45:38,588] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:45:38,590] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:45:38,591] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:45:38,595] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:45:38,597] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:45:38,598] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:45:38,599] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:45:38,603] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:45:38,605] {spark_submit.py:485} INFO - 22/07/06 07:45:38 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220706072954-0000/14 on worker-20220706072615-172.20.0.4-34743 (172.20.0.4:34743) with 1 core(s)
[2022-07-06 07:45:38,606] {spark_submit.py:485} INFO - 22/07/06 07:45:38 INFO StandaloneSchedulerBackend: Granted executor ID app-20220706072954-0000/14 on hostPort 172.20.0.4:34743 with 1 core(s), 1024.0 MiB RAM
[2022-07-06 07:45:38,608] {spark_submit.py:485} INFO - 22/07/06 07:45:38 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220706072954-0000/15 on worker-20220706072615-172.20.0.5-44257 (172.20.0.5:44257) with 1 core(s)
[2022-07-06 07:45:38,610] {spark_submit.py:485} INFO - 22/07/06 07:45:38 INFO StandaloneSchedulerBackend: Granted executor ID app-20220706072954-0000/15 on hostPort 172.20.0.5:44257 with 1 core(s), 1024.0 MiB RAM
[2022-07-06 07:45:38,852] {spark_submit.py:485} INFO - 22/07/06 07:45:38 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:45:38,853] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:45:38,855] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:45:38,857] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:45:38,858] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:45:38,861] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:45:38,863] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:45:38,867] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:45:38,869] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:45:38,879] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:45:38,880] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:45:38,882] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:45:38,883] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:45:38,885] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:45:38,887] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:45:38,888] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:45:38,890] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:45:38,893] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:45:38,895] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:45:38,897] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:45:38,902] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:45:38,903] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:45:38,905] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:45:38,906] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:45:38,908] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:45:38,910] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:45:38,911] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:45:38,913] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:45:38,915] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:45:38,916] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:45:38,918] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:45:38,920] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:45:38,921] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:45:38,925] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:45:38,926] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:45:38,928] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:45:38,930] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:45:38,935] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:45:38,937] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:45:38,941] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:45:38,944] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:45:38,947] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:45:38,950] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:45:38,951] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:45:38,954] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:45:38,955] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:45:38,957] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:45:38,964] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:45:38,965] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:45:38,967] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:45:38,969] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:45:38,971] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:45:38,975] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:45:38,979] {spark_submit.py:485} INFO - 22/07/06 07:45:38 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:45:38,982] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:45:38,988] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:45:38,990] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:45:38,991] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:45:38,994] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:45:39,003] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:45:39,010] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:45:39,015] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:45:39,017] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:45:39,020] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:45:39,022] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:45:39,024] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:45:39,026] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:45:39,028] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:45:39,030] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:45:39,032] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:45:39,034] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:45:39,035] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:45:39,038] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:45:39,040] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:45:39,041] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:45:39,043] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:45:39,045] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:45:39,047] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:45:39,051] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:45:39,053] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:45:39,055] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:45:39,056] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:45:39,058] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:45:39,060] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:45:39,061] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:45:39,063] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:45:39,064] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:45:39,066] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:45:39,068] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:45:39,070] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:45:39,072] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:45:39,073] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:45:39,075] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:45:39,077] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:45:39,078] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:45:39,080] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:45:39,082] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:45:39,083] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:45:39,086] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:45:39,088] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:45:39,090] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:45:39,092] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:45:39,093] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:45:39,095] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:45:39,096] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:45:39,098] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:45:46,453] {spark_submit.py:485} INFO - 22/07/06 07:45:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:46:01,458] {spark_submit.py:485} INFO - 22/07/06 07:46:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:46:16,455] {spark_submit.py:485} INFO - 22/07/06 07:46:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:46:31,455] {spark_submit.py:485} INFO - 22/07/06 07:46:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:46:46,454] {spark_submit.py:485} INFO - 22/07/06 07:46:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:47:01,454] {spark_submit.py:485} INFO - 22/07/06 07:47:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:47:16,458] {spark_submit.py:485} INFO - 22/07/06 07:47:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:47:31,454] {spark_submit.py:485} INFO - 22/07/06 07:47:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:47:46,459] {spark_submit.py:485} INFO - 22/07/06 07:47:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:47:48,748] {spark_submit.py:485} INFO - 22/07/06 07:47:48 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:47:48,752] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:47:48,754] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:47:48,759] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:47:48,766] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:47:48,769] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:47:48,773] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:47:48,775] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:47:48,778] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:47:48,781] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:47:48,783] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:47:48,785] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:47:48,788] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:47:48,791] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:47:48,793] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:47:48,795] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:47:48,798] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:47:48,801] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:47:48,804] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:47:48,815] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:47:48,830] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:47:48,836] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:47:48,839] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:47:48,842] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:47:48,845] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:47:48,848] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:47:48,850] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:47:48,852] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:47:48,855] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:47:48,863] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:47:48,865] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:47:48,867] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:47:48,870] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:47:48,873] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:47:48,878] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:47:48,885] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:47:48,888] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:47:48,891] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:47:48,896] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:47:48,899] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:47:48,902] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:47:48,906] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:47:48,911] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:47:48,914] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:47:48,917] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:47:48,921] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:47:48,924] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:47:48,928] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:47:48,930] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:47:48,933] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:47:48,936] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:47:48,939] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:47:48,942] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:47:48,946] {spark_submit.py:485} INFO - 22/07/06 07:47:48 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220706072954-0000/16 on worker-20220706072615-172.20.0.4-34743 (172.20.0.4:34743) with 1 core(s)
[2022-07-06 07:47:48,950] {spark_submit.py:485} INFO - 22/07/06 07:47:48 INFO StandaloneSchedulerBackend: Granted executor ID app-20220706072954-0000/16 on hostPort 172.20.0.4:34743 with 1 core(s), 1024.0 MiB RAM
[2022-07-06 07:47:48,953] {spark_submit.py:485} INFO - 22/07/06 07:47:48 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:47:48,957] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:47:48,960] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:47:48,963] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:47:48,965] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:47:48,969] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:47:48,973] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:47:48,975] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:47:48,979] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:47:48,981] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:47:48,984] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:47:48,987] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:47:48,990] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:47:48,993] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:47:48,995] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:47:49,002] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:47:49,007] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:47:49,011] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:47:49,015] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:47:49,019] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:47:49,024] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:47:49,031] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:47:49,035] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:47:49,040] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:47:49,046] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:47:49,055] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:47:49,059] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:47:49,067] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:47:49,072] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:47:49,074] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:47:49,080] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:47:49,083] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:47:49,086] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:47:49,090] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:47:49,092] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:47:49,096] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:47:49,099] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:47:49,103] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:47:49,106] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:47:49,109] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:47:49,111] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:47:49,113] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:47:49,116] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:47:49,118] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:47:49,122] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:47:49,125] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:47:49,127] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:47:49,130] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:47:49,133] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:47:49,135] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:47:49,138] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:47:49,139] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:47:49,143] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:47:49,145] {spark_submit.py:485} INFO - 22/07/06 07:47:48 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220706072954-0000/17 on worker-20220706072615-172.20.0.5-44257 (172.20.0.5:44257) with 1 core(s)
[2022-07-06 07:47:49,148] {spark_submit.py:485} INFO - 22/07/06 07:47:48 INFO StandaloneSchedulerBackend: Granted executor ID app-20220706072954-0000/17 on hostPort 172.20.0.5:44257 with 1 core(s), 1024.0 MiB RAM
[2022-07-06 07:47:49,177] {spark_submit.py:485} INFO - 22/07/06 07:47:49 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:47:49,180] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:47:49,182] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:47:49,186] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:47:49,189] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:47:49,191] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:47:49,193] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:47:49,195] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:47:49,197] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:47:49,199] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:47:49,201] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:47:49,202] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:47:49,204] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:47:49,207] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:47:49,210] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:47:49,213] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:47:49,214] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:47:49,216] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:47:49,219] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:47:49,221] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:47:49,224] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:47:49,225] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:47:49,228] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:47:49,231] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:47:49,233] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:47:49,234] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:47:49,237] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:47:49,239] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:47:49,241] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:47:49,243] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:47:49,248] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:47:49,249] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:47:49,253] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:47:49,255] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:47:49,257] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:47:49,259] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:47:49,262] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:47:49,272] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:47:49,276] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:47:49,292] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:47:49,300] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:47:49,301] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:47:49,303] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:47:49,305] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:47:49,306] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:47:49,309] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:47:49,311] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:47:49,313] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:47:49,315] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:47:49,317] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:47:49,318] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:47:49,320] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:47:49,323] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:47:49,349] {spark_submit.py:485} INFO - 22/07/06 07:47:49 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:47:49,351] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:47:49,352] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:47:49,354] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:47:49,356] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:47:49,358] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:47:49,375] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:47:49,386] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:47:49,388] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:47:49,389] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:47:49,391] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:47:49,392] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:47:49,400] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:47:49,402] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:47:49,404] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:47:49,405] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:47:49,407] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:47:49,409] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:47:49,413] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:47:49,418] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:47:49,419] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:47:49,421] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:47:49,428] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:47:49,430] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:47:49,432] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:47:49,434] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:47:49,435] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:47:49,437] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:47:49,438] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:47:49,440] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:47:49,442] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:47:49,443] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:47:49,446] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:47:49,449] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:47:49,450] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:47:49,452] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:47:49,453] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:47:49,455] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:47:49,456] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:47:49,458] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:47:49,460] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:47:49,469] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:47:49,471] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:47:49,472] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:47:49,474] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:47:49,475] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:47:49,477] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:47:49,479] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:47:49,480] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:47:49,482] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:47:49,483] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:47:49,485] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:47:49,487] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:48:01,456] {spark_submit.py:485} INFO - 22/07/06 07:48:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:48:16,461] {spark_submit.py:485} INFO - 22/07/06 07:48:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:48:31,457] {spark_submit.py:485} INFO - 22/07/06 07:48:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:48:46,456] {spark_submit.py:485} INFO - 22/07/06 07:48:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:49:01,454] {spark_submit.py:485} INFO - 22/07/06 07:49:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:49:16,593] {spark_submit.py:485} INFO - 22/07/06 07:49:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:49:31,456] {spark_submit.py:485} INFO - 22/07/06 07:49:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:49:46,454] {spark_submit.py:485} INFO - 22/07/06 07:49:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:49:57,499] {spark_submit.py:485} INFO - 22/07/06 07:49:57 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:49:57,501] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:49:57,512] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:49:57,514] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:49:57,518] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:49:57,529] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:49:57,531] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:49:57,533] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:49:57,537] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:49:57,564] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:49:57,567] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:49:57,574] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:49:57,579] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:49:57,582] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:49:57,586] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:49:57,595] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:49:57,598] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:49:57,600] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:49:57,619] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:49:57,622] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:49:57,626] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:49:57,628] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:49:57,630] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:49:57,634] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:49:57,641] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:49:57,649] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:49:57,652] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:49:57,665] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:49:57,675] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:49:57,680] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:49:57,689] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:49:57,698] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:49:57,715] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:49:57,719] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:49:57,732] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:49:57,736] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:49:57,739] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:49:57,741] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:49:57,749] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:49:57,766] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:49:57,780] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:49:57,785] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:49:57,794] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:49:57,802] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:49:57,814] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:49:57,833] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:49:57,845] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:49:57,849] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:49:57,854] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:49:57,867] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:49:57,878] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:49:57,880] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:49:57,882] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:49:57,957] {spark_submit.py:485} INFO - 22/07/06 07:49:57 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220706072954-0000/18 on worker-20220706072615-172.20.0.4-34743 (172.20.0.4:34743) with 1 core(s)
[2022-07-06 07:49:57,965] {spark_submit.py:485} INFO - 22/07/06 07:49:57 INFO StandaloneSchedulerBackend: Granted executor ID app-20220706072954-0000/18 on hostPort 172.20.0.4:34743 with 1 core(s), 1024.0 MiB RAM
[2022-07-06 07:49:58,323] {spark_submit.py:485} INFO - 22/07/06 07:49:58 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:49:58,330] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:49:58,333] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:49:58,338] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:49:58,342] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:49:58,353] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:49:58,357] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:49:58,363] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:49:58,371] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:49:58,389] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:49:58,396] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:49:58,399] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:49:58,402] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:49:58,408] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:49:58,411] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:49:58,416] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:49:58,420] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:49:58,434] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:49:58,436] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:49:58,450] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:49:58,455] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:49:58,461] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:49:58,467] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:49:58,471] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:49:58,481] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:49:58,484] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:49:58,485] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:49:58,487] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:49:58,492] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:49:58,494] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:49:58,499] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:49:58,507] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:49:58,515] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:49:58,528] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:49:58,557] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:49:58,558] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:49:58,561] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:49:58,562] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:49:58,567] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:49:58,568] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:49:58,584] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:49:58,587] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:49:58,619] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:49:58,640] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:49:58,673] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:49:58,688] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:49:58,698] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:49:58,703] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:49:58,718] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:49:58,721] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:49:58,724] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:49:58,728] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:49:58,750] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:49:58,752] {spark_submit.py:485} INFO - 22/07/06 07:49:58 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:49:58,754] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:49:58,756] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:49:58,759] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:49:58,762] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:49:58,764] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:49:58,767] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:49:58,770] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:49:58,775] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:49:58,784] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:49:58,786] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:49:58,788] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:49:58,790] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:49:58,793] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:49:58,795] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:49:58,800] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:49:58,802] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:49:58,818] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:49:58,871] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:49:58,888] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:49:58,890] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:49:58,892] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:49:58,896] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:49:58,899] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:49:58,903] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:49:58,908] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:49:58,910] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:49:58,914] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:49:58,917] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:49:58,918] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:49:58,920] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:49:58,923] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:49:58,925] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:49:58,928] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:49:58,930] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:49:58,933] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:49:58,936] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:49:58,938] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:49:58,940] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:49:58,942] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:49:58,944] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:49:58,947] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:49:58,958] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:49:58,968] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:49:58,972] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:49:58,989] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:49:58,991] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:49:58,998] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:49:59,001] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:49:59,003] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:49:59,010] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:49:59,021] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:49:59,025] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:49:59,026] {spark_submit.py:485} INFO - 22/07/06 07:49:58 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220706072954-0000/19 on worker-20220706072615-172.20.0.5-44257 (172.20.0.5:44257) with 1 core(s)
[2022-07-06 07:49:59,028] {spark_submit.py:485} INFO - 22/07/06 07:49:58 INFO StandaloneSchedulerBackend: Granted executor ID app-20220706072954-0000/19 on hostPort 172.20.0.5:44257 with 1 core(s), 1024.0 MiB RAM
[2022-07-06 07:49:59,743] {spark_submit.py:485} INFO - 22/07/06 07:49:59 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:49:59,756] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:49:59,773] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:49:59,803] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:49:59,811] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:49:59,835] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:49:59,839] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:49:59,867] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:49:59,871] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:50:00,285] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:50:00,293] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:50:00,307] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:50:00,355] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:50:00,371] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:50:00,388] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:50:00,393] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:50:00,411] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:50:00,432] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:50:00,445] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:50:00,473] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:50:00,559] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:50:00,579] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:50:00,593] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:50:00,600] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:50:00,603] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:50:00,608] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:50:00,612] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:50:00,627] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:50:00,646] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:50:00,660] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:50:00,664] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:50:00,713] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:50:00,739] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:50:00,747] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:50:00,765] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:50:00,787] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:50:00,791] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:50:00,798] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:50:00,811] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:50:00,825] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:50:00,842] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:50:00,844] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:50:00,862] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:50:00,885] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:50:00,904] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:50:00,907] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:50:00,913] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:50:00,933] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:50:00,939] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:50:00,978] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:50:00,980] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:50:00,993] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:50:00,996] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:50:01,454] {spark_submit.py:485} INFO - 22/07/06 07:50:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:50:16,454] {spark_submit.py:485} INFO - 22/07/06 07:50:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:50:31,455] {spark_submit.py:485} INFO - 22/07/06 07:50:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:50:46,460] {spark_submit.py:485} INFO - 22/07/06 07:50:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:51:01,453] {spark_submit.py:485} INFO - 22/07/06 07:51:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:51:16,454] {spark_submit.py:485} INFO - 22/07/06 07:51:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:51:31,731] {spark_submit.py:485} INFO - 22/07/06 07:51:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:51:46,462] {spark_submit.py:485} INFO - 22/07/06 07:51:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:52:01,454] {spark_submit.py:485} INFO - 22/07/06 07:52:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:52:13,841] {spark_submit.py:485} INFO - 22/07/06 07:52:13 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:52:13,848] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:52:13,850] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:52:13,854] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:52:13,857] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:52:13,862] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:52:13,865] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:52:13,873] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:52:13,885] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:52:13,892] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:52:13,902] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:52:13,907] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:52:13,925] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:52:13,931] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:52:13,936] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:52:13,943] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:52:13,947] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:52:13,956] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:52:13,962] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:52:13,964] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:52:13,997] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:52:13,999] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:52:14,006] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:52:14,009] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:52:14,013] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:52:14,017] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:52:14,022] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:52:14,027] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:52:14,043] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:52:14,079] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:52:14,113] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:52:14,202] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:52:14,208] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:52:14,211] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:52:14,214] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:52:14,219] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:52:14,254] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:52:14,260] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:52:14,269] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:52:14,277] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:52:14,310] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:52:14,318] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:52:14,337] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:52:14,341] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:52:14,354] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:52:14,361] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:52:14,368] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:52:14,372] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:52:14,375] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:52:14,378] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:52:14,381] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:52:14,385] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:52:14,395] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:52:14,423] {spark_submit.py:485} INFO - 22/07/06 07:52:13 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220706072954-0000/20 on worker-20220706072615-172.20.0.4-34743 (172.20.0.4:34743) with 1 core(s)
[2022-07-06 07:52:14,447] {spark_submit.py:485} INFO - 22/07/06 07:52:13 INFO StandaloneSchedulerBackend: Granted executor ID app-20220706072954-0000/20 on hostPort 172.20.0.4:34743 with 1 core(s), 1024.0 MiB RAM
[2022-07-06 07:52:15,515] {spark_submit.py:485} INFO - 22/07/06 07:52:15 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:52:15,519] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:52:15,525] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:52:15,529] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:52:15,546] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:52:15,548] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:52:15,551] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:52:15,555] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:52:15,620] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:52:15,655] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:52:15,660] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:52:15,666] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:52:15,669] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:52:15,679] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:52:16,583] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:52:16,611] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:52:16,619] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:52:16,626] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:52:16,631] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:52:16,636] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:52:16,638] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:52:16,651] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:52:16,653] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:52:16,661] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:52:16,664] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:52:16,677] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:52:16,686] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:52:16,697] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:52:16,699] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:52:16,707] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:52:16,711] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:52:16,741] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:52:16,769] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:52:16,772] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:52:16,774] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:52:16,789] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:52:16,826] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:52:16,838] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:52:16,843] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:52:16,854] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:52:16,857] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:52:16,863] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:52:16,867] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:52:16,869] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:52:16,872] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:52:16,876] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:52:16,884] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:52:16,886] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:52:16,889] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:52:16,892] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:52:16,898] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:52:16,900] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:52:16,906] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:52:16,909] {spark_submit.py:485} INFO - 22/07/06 07:52:15 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:52:16,911] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:52:16,915] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:52:16,921] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:52:16,923] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:52:16,925] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:52:16,927] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:52:16,940] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:52:16,947] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:52:16,980] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:52:17,019] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:52:17,027] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:52:17,040] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:52:17,043] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:52:17,057] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:52:17,064] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:52:17,083] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:52:17,087] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:52:17,106] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:52:17,110] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:52:17,113] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:52:17,116] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:52:17,129] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:52:17,131] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:52:17,133] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:52:17,136] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:52:17,139] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:52:17,141] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:52:17,143] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:52:17,157] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:52:17,159] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:52:17,162] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:52:17,165] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:52:17,168] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:52:17,170] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:52:17,172] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:52:17,174] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:52:17,180] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:52:17,184] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:52:17,186] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:52:17,188] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:52:17,191] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:52:17,193] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:52:17,195] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:52:17,198] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:52:17,200] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:52:17,210] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:52:17,212] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:52:17,214] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:52:17,215] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:52:17,218] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:52:17,220] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:52:17,221] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:52:17,223] {spark_submit.py:485} INFO - 22/07/06 07:52:15 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220706072954-0000/21 on worker-20220706072615-172.20.0.5-44257 (172.20.0.5:44257) with 1 core(s)
[2022-07-06 07:52:17,225] {spark_submit.py:485} INFO - 22/07/06 07:52:15 INFO StandaloneSchedulerBackend: Granted executor ID app-20220706072954-0000/21 on hostPort 172.20.0.5:44257 with 1 core(s), 1024.0 MiB RAM
[2022-07-06 07:52:17,227] {spark_submit.py:485} INFO - 22/07/06 07:52:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:52:17,229] {spark_submit.py:485} INFO - 22/07/06 07:52:16 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:52:17,231] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:52:17,233] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:52:17,235] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:52:17,237] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:52:17,244] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:52:17,246] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:52:17,250] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:52:17,253] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:52:17,256] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:52:17,258] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:52:17,261] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:52:17,269] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:52:17,283] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:52:17,285] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:52:17,287] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:52:17,290] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:52:17,291] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:52:17,293] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:52:17,294] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:52:17,303] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:52:17,305] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:52:17,307] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:52:17,308] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:52:17,309] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:52:17,311] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:52:17,313] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:52:17,314] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:52:17,316] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:52:17,317] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:52:17,319] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:52:17,320] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:52:17,329] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:52:17,331] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:52:17,332] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:52:17,335] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:52:17,337] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:52:17,339] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:52:17,340] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:52:17,341] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:52:17,343] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:52:17,344] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:52:17,346] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:52:17,347] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:52:17,352] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:52:17,354] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:52:17,355] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:52:17,357] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:52:17,361] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:52:17,363] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:52:17,365] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:52:17,368] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:52:17,371] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:52:31,454] {spark_submit.py:485} INFO - 22/07/06 07:52:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:52:46,454] {spark_submit.py:485} INFO - 22/07/06 07:52:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:53:01,454] {spark_submit.py:485} INFO - 22/07/06 07:53:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:53:16,453] {spark_submit.py:485} INFO - 22/07/06 07:53:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:53:31,455] {spark_submit.py:485} INFO - 22/07/06 07:53:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:53:46,454] {spark_submit.py:485} INFO - 22/07/06 07:53:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:54:01,457] {spark_submit.py:485} INFO - 22/07/06 07:54:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:54:16,455] {spark_submit.py:485} INFO - 22/07/06 07:54:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:54:31,454] {spark_submit.py:485} INFO - 22/07/06 07:54:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:54:36,421] {spark_submit.py:485} INFO - 22/07/06 07:54:36 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:54:36,424] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:54:36,451] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:54:36,474] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:54:36,478] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:54:36,488] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:54:36,496] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:54:36,505] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:54:36,539] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:54:36,543] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:54:36,559] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:54:36,563] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:54:36,570] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:54:36,573] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:54:36,576] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:54:36,592] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:54:36,602] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:54:36,606] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:54:36,608] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:54:36,611] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:54:36,617] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:54:36,620] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:54:36,623] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:54:36,627] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:54:36,640] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:54:36,642] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:54:36,644] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:54:36,646] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:54:36,648] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:54:36,650] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:54:36,657] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:54:36,660] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:54:36,683] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:54:36,686] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:54:36,693] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:54:36,695] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:54:36,699] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:54:36,702] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:54:36,706] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:54:36,709] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:54:36,713] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:54:36,728] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:54:36,740] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:54:36,747] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:54:36,758] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:54:36,761] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:54:36,772] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:54:36,774] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:54:36,777] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:54:36,779] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:54:36,797] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:54:36,802] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:54:36,806] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:54:36,809] {spark_submit.py:485} INFO - 22/07/06 07:54:36 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220706072954-0000/22 on worker-20220706072615-172.20.0.4-34743 (172.20.0.4:34743) with 1 core(s)
[2022-07-06 07:54:36,811] {spark_submit.py:485} INFO - 22/07/06 07:54:36 INFO StandaloneSchedulerBackend: Granted executor ID app-20220706072954-0000/22 on hostPort 172.20.0.4:34743 with 1 core(s), 1024.0 MiB RAM
[2022-07-06 07:54:36,814] {spark_submit.py:485} INFO - 22/07/06 07:54:36 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:54:36,816] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:54:36,818] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:54:36,823] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:54:36,827] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:54:36,829] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:54:36,833] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:54:36,835] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:54:36,838] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:54:36,841] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:54:36,843] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:54:36,845] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:54:36,849] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:54:36,851] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:54:36,853] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:54:36,856] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:54:36,859] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:54:36,863] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:54:36,867] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:54:36,869] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:54:36,872] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:54:36,874] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:54:36,878] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:54:36,883] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:54:36,885] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:54:36,887] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:54:36,890] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:54:36,893] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:54:36,895] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:54:36,900] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:54:36,902] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:54:36,906] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:54:36,909] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:54:36,911] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:54:36,916] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:54:36,917] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:54:36,919] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:54:36,922] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:54:36,925] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:54:36,927] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:54:36,928] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:54:36,931] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:54:36,954] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:54:36,959] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:54:36,960] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:54:36,962] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:54:36,964] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:54:36,975] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:54:36,977] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:54:36,980] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:54:36,982] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:54:36,983] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:54:36,985] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:54:37,046] {spark_submit.py:485} INFO - 22/07/06 07:54:37 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:54:37,049] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:54:37,051] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:54:37,053] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:54:37,056] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:54:37,058] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:54:37,060] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:54:37,070] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:54:37,073] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:54:37,076] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:54:37,078] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:54:37,082] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:54:37,084] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:54:37,088] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:54:37,091] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:54:37,093] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:54:37,095] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:54:37,098] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:54:37,104] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:54:37,107] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:54:37,111] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:54:37,113] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:54:37,116] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:54:37,118] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:54:37,122] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:54:37,125] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:54:37,128] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:54:37,130] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:54:37,132] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:54:37,135] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:54:37,138] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:54:37,142] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:54:37,144] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:54:37,147] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:54:37,150] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:54:37,153] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:54:37,157] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:54:37,159] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:54:37,161] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:54:37,163] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:54:37,166] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:54:37,169] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:54:37,172] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:54:37,174] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:54:37,177] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:54:37,179] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:54:37,181] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:54:37,184] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:54:37,187] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:54:37,190] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:54:37,193] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:54:37,197] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:54:37,199] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:54:37,203] {spark_submit.py:485} INFO - 22/07/06 07:54:37 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220706072954-0000/23 on worker-20220706072615-172.20.0.5-44257 (172.20.0.5:44257) with 1 core(s)
[2022-07-06 07:54:37,206] {spark_submit.py:485} INFO - 22/07/06 07:54:37 INFO StandaloneSchedulerBackend: Granted executor ID app-20220706072954-0000/23 on hostPort 172.20.0.5:44257 with 1 core(s), 1024.0 MiB RAM
[2022-07-06 07:54:37,448] {spark_submit.py:485} INFO - 22/07/06 07:54:37 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-07-06 07:54:37,462] {spark_submit.py:485} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = -1971851081955655249, local class serialVersionUID = 1654279024112373855
[2022-07-06 07:54:37,466] {spark_submit.py:485} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-07-06 07:54:37,470] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-07-06 07:54:37,473] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-07-06 07:54:37,479] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-07-06 07:54:37,484] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-07-06 07:54:37,488] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-07-06 07:54:37,491] {spark_submit.py:485} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-07-06 07:54:37,494] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-07-06 07:54:37,497] {spark_submit.py:485} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-07-06 07:54:37,504] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:292)
[2022-07-06 07:54:37,515] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:54:37,519] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:345)
[2022-07-06 07:54:37,532] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:291)
[2022-07-06 07:54:37,534] {spark_submit.py:485} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-07-06 07:54:37,536] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:291)
[2022-07-06 07:54:37,538] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:640)
[2022-07-06 07:54:37,540] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:691)
[2022-07-06 07:54:37,542] {spark_submit.py:485} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)
[2022-07-06 07:54:37,543] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
[2022-07-06 07:54:37,545] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-07-06 07:54:37,547] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-07-06 07:54:37,553] {spark_submit.py:485} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-07-06 07:54:37,555] {spark_submit.py:485} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-07-06 07:54:37,556] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:54:37,558] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:54:37,560] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:54:37,561] {spark_submit.py:485} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-07-06 07:54:37,564] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:54:37,567] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:54:37,568] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:54:37,571] {spark_submit.py:485} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
[2022-07-06 07:54:37,573] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:54:37,576] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:54:37,586] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:54:37,588] {spark_submit.py:485} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-07-06 07:54:37,590] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:54:37,591] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:54:37,593] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-07-06 07:54:37,595] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-07-06 07:54:37,597] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-07-06 07:54:37,598] {spark_submit.py:485} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-07-06 07:54:37,600] {spark_submit.py:485} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-07-06 07:54:37,601] {spark_submit.py:485} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2022-07-06 07:54:37,603] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2022-07-06 07:54:37,609] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2022-07-06 07:54:37,610] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2022-07-06 07:54:37,612] {spark_submit.py:485} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-07-06 07:54:37,613] {spark_submit.py:485} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-07-06 07:54:37,616] {spark_submit.py:485} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-07-06 07:54:37,618] {spark_submit.py:485} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-07-06 07:54:37,620] {spark_submit.py:485} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-07-06 07:54:46,454] {spark_submit.py:485} INFO - 22/07/06 07:54:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:55:01,453] {spark_submit.py:485} INFO - 22/07/06 07:55:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:55:16,454] {spark_submit.py:485} INFO - 22/07/06 07:55:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:55:31,455] {spark_submit.py:485} INFO - 22/07/06 07:55:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:55:46,456] {spark_submit.py:485} INFO - 22/07/06 07:55:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:55:54,129] {local_task_job.py:82} ERROR - Received SIGTERM. Terminating subprocesses
[2022-07-06 07:55:56,348] {process_utils.py:124} INFO - Sending Signals.SIGTERM to group 52. PIDs of all processes in the group: [63, 139, 52]
[2022-07-06 07:55:57,216] {process_utils.py:75} INFO - Sending the signal Signals.SIGTERM to group 52
[2022-07-06 07:55:57,786] {taskinstance.py:1408} ERROR - Received SIGTERM. Terminating subprocesses.
[2022-07-06 07:56:00,838] {spark_submit.py:617} INFO - Sending kill signal to spark-submit
[2022-07-06 07:56:02,468] {process_utils.py:70} INFO - Process psutil.Process(pid=139, status='terminated', started='07:29:37') (139) terminated with exit code None

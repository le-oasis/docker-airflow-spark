[2022-07-06 07:28:57,821] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: spark-test.spark_job manual__2022-07-06T07:18:53.893064+00:00 [queued]>
[2022-07-06 07:28:57,910] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: spark-test.spark_job manual__2022-07-06T07:18:53.893064+00:00 [queued]>
[2022-07-06 07:28:57,912] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2022-07-06 07:28:57,921] {taskinstance.py:1239} INFO - Starting attempt 2 of 2
[2022-07-06 07:28:57,924] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2022-07-06 07:28:57,973] {taskinstance.py:1259} INFO - Executing <Task(SparkSubmitOperator): spark_job> on 2022-07-06 07:18:53.893064+00:00
[2022-07-06 07:28:57,993] {standard_task_runner.py:52} INFO - Started process 51 to run task
[2022-07-06 07:28:58,007] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'spark-test', 'spark_job', 'manual__2022-07-06T07:18:53.893064+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/spark-test.py', '--cfg-path', '/tmp/tmp2c7kz5zv', '--error-file', '/tmp/tmp3izi5x4n']
[2022-07-06 07:28:58,010] {standard_task_runner.py:77} INFO - Job 4: Subtask spark_job
[2022-07-06 07:28:58,336] {logging_mixin.py:109} INFO - Running <TaskInstance: spark-test.spark_job manual__2022-07-06T07:18:53.893064+00:00 [running]> on host 538ddc85acbc
[2022-07-06 07:29:02,291] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=***@***.com
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=spark-test
AIRFLOW_CTX_TASK_ID=spark_job
AIRFLOW_CTX_EXECUTION_DATE=2022-07-06T07:18:53.893064+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2022-07-06T07:18:53.893064+00:00
[2022-07-06 07:29:02,465] {base.py:79} INFO - Using connection to: id: spark_connection. Host: spark://spark, Port: 7077, Schema: , Login: , Password: None, extra: {'queue': 'root.default'}
[2022-07-06 07:29:02,549] {spark_submit.py:334} INFO - Spark-Submit cmd: spark-submit --master spark://spark:7077 --conf spark.master=spark://spark:7077 --name Spark Hello World --verbose --queue root.default /usr/local/spark/app/hello-world.py /usr/local/spark/resources/data/***.cfg
[2022-07-06 07:29:32,391] {spark_submit.py:485} INFO - Using properties file: null
[2022-07-06 07:29:32,920] {spark_submit.py:485} INFO - WARNING: An illegal reflective access operation has occurred
[2022-07-06 07:29:32,922] {spark_submit.py:485} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/spark/spark-3.0.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2022-07-06 07:29:32,925] {spark_submit.py:485} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2022-07-06 07:29:32,927] {spark_submit.py:485} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2022-07-06 07:29:32,935] {spark_submit.py:485} INFO - WARNING: All illegal access operations will be denied in a future release
[2022-07-06 07:29:33,197] {spark_submit.py:485} INFO - Parsed arguments:
[2022-07-06 07:29:33,199] {spark_submit.py:485} INFO - master                  spark://spark:7077
[2022-07-06 07:29:33,202] {spark_submit.py:485} INFO - deployMode              null
[2022-07-06 07:29:33,205] {spark_submit.py:485} INFO - executorMemory          null
[2022-07-06 07:29:33,208] {spark_submit.py:485} INFO - executorCores           null
[2022-07-06 07:29:33,216] {spark_submit.py:485} INFO - totalExecutorCores      null
[2022-07-06 07:29:33,220] {spark_submit.py:485} INFO - propertiesFile          null
[2022-07-06 07:29:33,222] {spark_submit.py:485} INFO - driverMemory            null
[2022-07-06 07:29:33,229] {spark_submit.py:485} INFO - driverCores             null
[2022-07-06 07:29:33,231] {spark_submit.py:485} INFO - driverExtraClassPath    null
[2022-07-06 07:29:33,233] {spark_submit.py:485} INFO - driverExtraLibraryPath  null
[2022-07-06 07:29:33,237] {spark_submit.py:485} INFO - driverExtraJavaOptions  null
[2022-07-06 07:29:33,244] {spark_submit.py:485} INFO - supervise               false
[2022-07-06 07:29:33,249] {spark_submit.py:485} INFO - queue                   root.default
[2022-07-06 07:29:33,252] {spark_submit.py:485} INFO - numExecutors            null
[2022-07-06 07:29:33,254] {spark_submit.py:485} INFO - files                   null
[2022-07-06 07:29:33,258] {spark_submit.py:485} INFO - pyFiles                 null
[2022-07-06 07:29:33,260] {spark_submit.py:485} INFO - archives                null
[2022-07-06 07:29:33,262] {spark_submit.py:485} INFO - mainClass               null
[2022-07-06 07:29:33,265] {spark_submit.py:485} INFO - primaryResource         file:/usr/local/spark/app/hello-world.py
[2022-07-06 07:29:33,267] {spark_submit.py:485} INFO - name                    Spark Hello World
[2022-07-06 07:29:33,269] {spark_submit.py:485} INFO - childArgs               [/usr/local/spark/resources/data/***.cfg]
[2022-07-06 07:29:33,271] {spark_submit.py:485} INFO - jars                    null
[2022-07-06 07:29:33,274] {spark_submit.py:485} INFO - packages                null
[2022-07-06 07:29:33,276] {spark_submit.py:485} INFO - packagesExclusions      null
[2022-07-06 07:29:33,278] {spark_submit.py:485} INFO - repositories            null
[2022-07-06 07:29:33,281] {spark_submit.py:485} INFO - verbose                 true
[2022-07-06 07:29:33,284] {spark_submit.py:485} INFO - 
[2022-07-06 07:29:33,286] {spark_submit.py:485} INFO - Spark properties used, including those specified through
[2022-07-06 07:29:33,288] {spark_submit.py:485} INFO - --conf and those from the properties file null:
[2022-07-06 07:29:33,291] {spark_submit.py:485} INFO - (spark.master,spark://spark:7077)
[2022-07-06 07:29:33,295] {spark_submit.py:485} INFO - 
[2022-07-06 07:29:33,298] {spark_submit.py:485} INFO - 
[2022-07-06 07:29:35,332] {spark_submit.py:485} INFO - 22/07/06 07:29:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-07-06 07:29:38,135] {spark_submit.py:485} INFO - Main class:
[2022-07-06 07:29:38,144] {spark_submit.py:485} INFO - org.apache.spark.deploy.PythonRunner
[2022-07-06 07:29:38,146] {spark_submit.py:485} INFO - Arguments:
[2022-07-06 07:29:38,152] {spark_submit.py:485} INFO - file:/usr/local/spark/app/hello-world.py
[2022-07-06 07:29:38,156] {spark_submit.py:485} INFO - null
[2022-07-06 07:29:38,160] {spark_submit.py:485} INFO - /usr/local/spark/resources/data/***.cfg
[2022-07-06 07:29:38,170] {spark_submit.py:485} INFO - Spark config:
[2022-07-06 07:29:38,173] {spark_submit.py:485} INFO - (spark.master,spark://spark:7077)
[2022-07-06 07:29:38,174] {spark_submit.py:485} INFO - (spark.app.name,Spark Hello World)
[2022-07-06 07:29:38,176] {spark_submit.py:485} INFO - (spark.submit.pyFiles,)
[2022-07-06 07:29:38,178] {spark_submit.py:485} INFO - (spark.submit.deployMode,client)
[2022-07-06 07:29:38,180] {spark_submit.py:485} INFO - Classpath elements:
[2022-07-06 07:29:38,183] {spark_submit.py:485} INFO - 
[2022-07-06 07:29:38,190] {spark_submit.py:485} INFO - 
[2022-07-06 07:29:38,192] {spark_submit.py:485} INFO - 
[2022-07-06 07:29:41,778] {spark_submit.py:485} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-07-06 07:29:41,867] {spark_submit.py:485} INFO - 22/07/06 07:29:41 INFO SparkContext: Running Spark version 3.0.1
[2022-07-06 07:29:42,180] {spark_submit.py:485} INFO - 22/07/06 07:29:42 INFO ResourceUtils: ==============================================================
[2022-07-06 07:29:42,190] {spark_submit.py:485} INFO - 22/07/06 07:29:42 INFO ResourceUtils: Resources for spark.driver:
[2022-07-06 07:29:42,192] {spark_submit.py:485} INFO - 
[2022-07-06 07:29:42,198] {spark_submit.py:485} INFO - 22/07/06 07:29:42 INFO ResourceUtils: ==============================================================
[2022-07-06 07:29:42,204] {spark_submit.py:485} INFO - 22/07/06 07:29:42 INFO SparkContext: Submitted application: Spark Hello World
[2022-07-06 07:29:42,977] {spark_submit.py:485} INFO - 22/07/06 07:29:42 INFO SecurityManager: Changing view acls to: default
[2022-07-06 07:29:42,981] {spark_submit.py:485} INFO - 22/07/06 07:29:42 INFO SecurityManager: Changing modify acls to: default
[2022-07-06 07:29:42,983] {spark_submit.py:485} INFO - 22/07/06 07:29:42 INFO SecurityManager: Changing view acls groups to:
[2022-07-06 07:29:42,988] {spark_submit.py:485} INFO - 22/07/06 07:29:42 INFO SecurityManager: Changing modify acls groups to:
[2022-07-06 07:29:42,993] {spark_submit.py:485} INFO - 22/07/06 07:29:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(default); groups with view permissions: Set(); users  with modify permissions: Set(default); groups with modify permissions: Set()
[2022-07-06 07:29:45,681] {spark_submit.py:485} INFO - 22/07/06 07:29:45 INFO Utils: Successfully started service 'sparkDriver' on port 39533.
[2022-07-06 07:29:45,900] {spark_submit.py:485} INFO - 22/07/06 07:29:45 INFO SparkEnv: Registering MapOutputTracker
[2022-07-06 07:29:46,028] {spark_submit.py:485} INFO - 22/07/06 07:29:46 INFO SparkEnv: Registering BlockManagerMaster
[2022-07-06 07:29:46,097] {spark_submit.py:485} INFO - 22/07/06 07:29:46 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-07-06 07:29:46,100] {spark_submit.py:485} INFO - 22/07/06 07:29:46 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-07-06 07:29:46,130] {spark_submit.py:485} INFO - 22/07/06 07:29:46 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-07-06 07:29:46,203] {spark_submit.py:485} INFO - 22/07/06 07:29:46 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-42c32b4d-94c9-4921-a3cc-676d2486c822
[2022-07-06 07:29:46,324] {spark_submit.py:485} INFO - 22/07/06 07:29:46 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2022-07-06 07:29:46,397] {spark_submit.py:485} INFO - 22/07/06 07:29:46 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-07-06 07:29:47,301] {spark_submit.py:485} INFO - 22/07/06 07:29:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2022-07-06 07:29:47,481] {spark_submit.py:485} INFO - 22/07/06 07:29:47 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2022-07-06 07:29:48,364] {spark_submit.py:485} INFO - 22/07/06 07:29:48 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://538ddc85acbc:4041
[2022-07-06 07:29:48,460] {spark_submit.py:485} INFO - 22/07/06 07:29:48 WARN SparkContext: Please ensure that the number of slots available on your executors is limited by the number of cores to task cpus and not another custom resource. If cores is not the limiting resource then dynamic allocation will not work properly!
[2022-07-06 07:29:49,575] {spark_submit.py:485} INFO - 22/07/06 07:29:49 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark:7077...
[2022-07-06 07:29:49,972] {spark_submit.py:485} INFO - 22/07/06 07:29:49 INFO TransportClientFactory: Successfully created connection to spark/172.20.0.7:7077 after 302 ms (0 ms spent in bootstraps)
[2022-07-06 07:29:55,106] {spark_submit.py:485} INFO - 22/07/06 07:29:55 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20220706072954-0001
[2022-07-06 07:29:55,367] {spark_submit.py:485} INFO - 22/07/06 07:29:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45411.
[2022-07-06 07:29:55,371] {spark_submit.py:485} INFO - 22/07/06 07:29:55 INFO NettyBlockTransferService: Server created on 538ddc85acbc:45411
[2022-07-06 07:29:55,405] {spark_submit.py:485} INFO - 22/07/06 07:29:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-07-06 07:29:55,864] {spark_submit.py:485} INFO - 22/07/06 07:29:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 538ddc85acbc, 45411, None)
[2022-07-06 07:29:56,122] {spark_submit.py:485} INFO - 22/07/06 07:29:56 INFO BlockManagerMasterEndpoint: Registering block manager 538ddc85acbc:45411 with 434.4 MiB RAM, BlockManagerId(driver, 538ddc85acbc, 45411, None)
[2022-07-06 07:29:56,157] {spark_submit.py:485} INFO - 22/07/06 07:29:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 538ddc85acbc, 45411, None)
[2022-07-06 07:29:56,169] {spark_submit.py:485} INFO - 22/07/06 07:29:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 538ddc85acbc, 45411, None)
[2022-07-06 07:30:07,232] {spark_submit.py:485} INFO - 22/07/06 07:30:07 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2022-07-06 07:30:07,378] {spark_submit.py:485} INFO - 22/07/06 07:30:07 INFO AsyncEventQueue: Process of event SparkListenerBlockManagerAdded(1657092596061,BlockManagerId(driver, 538ddc85acbc, 45411, None),455501414,Some(455501414),Some(0)) by listener AppStatusListener took 1.7597355s.
[2022-07-06 07:30:22,236] {spark_submit.py:485} INFO - 22/07/06 07:30:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 175.8 KiB, free 434.2 MiB)
[2022-07-06 07:30:24,893] {spark_submit.py:485} INFO - 22/07/06 07:30:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.1 KiB, free 434.2 MiB)
[2022-07-06 07:30:24,965] {spark_submit.py:485} INFO - 22/07/06 07:30:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 538ddc85acbc:45411 (size: 27.1 KiB, free: 434.4 MiB)
[2022-07-06 07:30:25,259] {spark_submit.py:485} INFO - 22/07/06 07:30:25 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0
[2022-07-06 07:30:26,671] {spark_submit.py:485} INFO - 22/07/06 07:30:26 INFO FileInputFormat: Total input files to process : 1
[2022-07-06 07:30:26,926] {spark_submit.py:485} INFO - 22/07/06 07:30:26 INFO SparkContext: Starting job: count at /usr/local/spark/app/hello-world.py:23
[2022-07-06 07:30:27,020] {spark_submit.py:485} INFO - 22/07/06 07:30:27 INFO DAGScheduler: Got job 0 (count at /usr/local/spark/app/hello-world.py:23) with 2 output partitions
[2022-07-06 07:30:27,026] {spark_submit.py:485} INFO - 22/07/06 07:30:27 INFO DAGScheduler: Final stage: ResultStage 0 (count at /usr/local/spark/app/hello-world.py:23)
[2022-07-06 07:30:27,037] {spark_submit.py:485} INFO - 22/07/06 07:30:27 INFO DAGScheduler: Parents of final stage: List()
[2022-07-06 07:30:27,111] {spark_submit.py:485} INFO - 22/07/06 07:30:27 INFO DAGScheduler: Missing parents: List()
[2022-07-06 07:30:27,225] {spark_submit.py:485} INFO - 22/07/06 07:30:27 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /usr/local/spark/app/hello-world.py:23), which has no missing parents
[2022-07-06 07:30:29,322] {spark_submit.py:485} INFO - 22/07/06 07:30:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.2 KiB, free 434.2 MiB)
[2022-07-06 07:30:29,538] {spark_submit.py:485} INFO - 22/07/06 07:30:29 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 434.2 MiB)
[2022-07-06 07:30:29,634] {spark_submit.py:485} INFO - 22/07/06 07:30:29 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 538ddc85acbc:45411 (size: 4.8 KiB, free: 434.4 MiB)
[2022-07-06 07:30:29,809] {spark_submit.py:485} INFO - 22/07/06 07:30:29 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
[2022-07-06 07:30:30,294] {spark_submit.py:485} INFO - 22/07/06 07:30:30 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1657092627111,WrappedArray(org.apache.spark.scheduler.StageInfo@7eb13ec5),{spark.rdd.scope={"id":"1","name":"collect"}, callSite.short=count at /usr/local/spark/app/hello-world.py:23, spark.rdd.scope.noOverride=true}) by listener AppStatusListener took 3.1068425s.
[2022-07-06 07:30:30,752] {spark_submit.py:485} INFO - 22/07/06 07:30:30 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at count at /usr/local/spark/app/hello-world.py:23) (first 15 tasks are for partitions Vector(0, 1))
[2022-07-06 07:30:30,774] {spark_submit.py:485} INFO - 22/07/06 07:30:30 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
[2022-07-06 07:30:46,127] {spark_submit.py:485} INFO - 22/07/06 07:30:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:31:01,127] {spark_submit.py:485} INFO - 22/07/06 07:31:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:31:16,128] {spark_submit.py:485} INFO - 22/07/06 07:31:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:31:31,126] {spark_submit.py:485} INFO - 22/07/06 07:31:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:31:46,128] {spark_submit.py:485} INFO - 22/07/06 07:31:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:32:01,125] {spark_submit.py:485} INFO - 22/07/06 07:32:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:32:16,127] {spark_submit.py:485} INFO - 22/07/06 07:32:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:32:31,125] {spark_submit.py:485} INFO - 22/07/06 07:32:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:32:46,126] {spark_submit.py:485} INFO - 22/07/06 07:32:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:33:01,127] {spark_submit.py:485} INFO - 22/07/06 07:33:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:33:16,126] {spark_submit.py:485} INFO - 22/07/06 07:33:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:33:31,126] {spark_submit.py:485} INFO - 22/07/06 07:33:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:33:46,133] {spark_submit.py:485} INFO - 22/07/06 07:33:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:34:01,126] {spark_submit.py:485} INFO - 22/07/06 07:34:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:34:16,127] {spark_submit.py:485} INFO - 22/07/06 07:34:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:34:31,127] {spark_submit.py:485} INFO - 22/07/06 07:34:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:34:46,126] {spark_submit.py:485} INFO - 22/07/06 07:34:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:35:01,127] {spark_submit.py:485} INFO - 22/07/06 07:35:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:35:16,127] {spark_submit.py:485} INFO - 22/07/06 07:35:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:35:31,126] {spark_submit.py:485} INFO - 22/07/06 07:35:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:35:46,127] {spark_submit.py:485} INFO - 22/07/06 07:35:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:36:01,127] {spark_submit.py:485} INFO - 22/07/06 07:36:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:36:16,127] {spark_submit.py:485} INFO - 22/07/06 07:36:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:36:31,128] {spark_submit.py:485} INFO - 22/07/06 07:36:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:36:46,126] {spark_submit.py:485} INFO - 22/07/06 07:36:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:37:01,125] {spark_submit.py:485} INFO - 22/07/06 07:37:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:37:16,127] {spark_submit.py:485} INFO - 22/07/06 07:37:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:37:31,129] {spark_submit.py:485} INFO - 22/07/06 07:37:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:37:46,133] {spark_submit.py:485} INFO - 22/07/06 07:37:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:38:01,128] {spark_submit.py:485} INFO - 22/07/06 07:38:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:38:16,126] {spark_submit.py:485} INFO - 22/07/06 07:38:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:38:31,126] {spark_submit.py:485} INFO - 22/07/06 07:38:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:38:46,127] {spark_submit.py:485} INFO - 22/07/06 07:38:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:39:01,130] {spark_submit.py:485} INFO - 22/07/06 07:39:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:39:16,126] {spark_submit.py:485} INFO - 22/07/06 07:39:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:39:31,126] {spark_submit.py:485} INFO - 22/07/06 07:39:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:39:46,126] {spark_submit.py:485} INFO - 22/07/06 07:39:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:40:01,127] {spark_submit.py:485} INFO - 22/07/06 07:40:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:40:16,126] {spark_submit.py:485} INFO - 22/07/06 07:40:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:40:31,126] {spark_submit.py:485} INFO - 22/07/06 07:40:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:40:46,127] {spark_submit.py:485} INFO - 22/07/06 07:40:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:41:01,126] {spark_submit.py:485} INFO - 22/07/06 07:41:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:41:16,128] {spark_submit.py:485} INFO - 22/07/06 07:41:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:41:31,126] {spark_submit.py:485} INFO - 22/07/06 07:41:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:41:46,126] {spark_submit.py:485} INFO - 22/07/06 07:41:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:42:01,128] {spark_submit.py:485} INFO - 22/07/06 07:42:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:42:16,127] {spark_submit.py:485} INFO - 22/07/06 07:42:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:42:31,126] {spark_submit.py:485} INFO - 22/07/06 07:42:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:42:46,126] {spark_submit.py:485} INFO - 22/07/06 07:42:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:43:01,127] {spark_submit.py:485} INFO - 22/07/06 07:43:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:43:16,129] {spark_submit.py:485} INFO - 22/07/06 07:43:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:43:31,125] {spark_submit.py:485} INFO - 22/07/06 07:43:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:43:46,127] {spark_submit.py:485} INFO - 22/07/06 07:43:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:44:01,127] {spark_submit.py:485} INFO - 22/07/06 07:44:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:44:16,126] {spark_submit.py:485} INFO - 22/07/06 07:44:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:44:31,137] {spark_submit.py:485} INFO - 22/07/06 07:44:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:44:46,127] {spark_submit.py:485} INFO - 22/07/06 07:44:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:45:01,129] {spark_submit.py:485} INFO - 22/07/06 07:45:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:45:16,128] {spark_submit.py:485} INFO - 22/07/06 07:45:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:45:31,126] {spark_submit.py:485} INFO - 22/07/06 07:45:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:45:46,126] {spark_submit.py:485} INFO - 22/07/06 07:45:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:46:01,129] {spark_submit.py:485} INFO - 22/07/06 07:46:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:46:16,127] {spark_submit.py:485} INFO - 22/07/06 07:46:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:46:31,132] {spark_submit.py:485} INFO - 22/07/06 07:46:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:46:46,127] {spark_submit.py:485} INFO - 22/07/06 07:46:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:47:01,126] {spark_submit.py:485} INFO - 22/07/06 07:47:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:47:16,181] {spark_submit.py:485} INFO - 22/07/06 07:47:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:47:31,130] {spark_submit.py:485} INFO - 22/07/06 07:47:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:47:46,126] {spark_submit.py:485} INFO - 22/07/06 07:47:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:48:01,128] {spark_submit.py:485} INFO - 22/07/06 07:48:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:48:16,209] {spark_submit.py:485} INFO - 22/07/06 07:48:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:48:31,126] {spark_submit.py:485} INFO - 22/07/06 07:48:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:48:46,129] {spark_submit.py:485} INFO - 22/07/06 07:48:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:49:01,128] {spark_submit.py:485} INFO - 22/07/06 07:49:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:49:16,570] {spark_submit.py:485} INFO - 22/07/06 07:49:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:49:31,126] {spark_submit.py:485} INFO - 22/07/06 07:49:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:49:46,126] {spark_submit.py:485} INFO - 22/07/06 07:49:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:50:01,126] {spark_submit.py:485} INFO - 22/07/06 07:50:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:50:16,127] {spark_submit.py:485} INFO - 22/07/06 07:50:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:50:31,127] {spark_submit.py:485} INFO - 22/07/06 07:50:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:50:46,126] {spark_submit.py:485} INFO - 22/07/06 07:50:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:51:01,126] {spark_submit.py:485} INFO - 22/07/06 07:51:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:51:16,131] {spark_submit.py:485} INFO - 22/07/06 07:51:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:51:31,132] {spark_submit.py:485} INFO - 22/07/06 07:51:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:51:46,127] {spark_submit.py:485} INFO - 22/07/06 07:51:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:52:01,126] {spark_submit.py:485} INFO - 22/07/06 07:52:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:52:16,493] {spark_submit.py:485} INFO - 22/07/06 07:52:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:52:31,126] {spark_submit.py:485} INFO - 22/07/06 07:52:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:52:46,127] {spark_submit.py:485} INFO - 22/07/06 07:52:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:53:01,126] {spark_submit.py:485} INFO - 22/07/06 07:53:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:53:16,126] {spark_submit.py:485} INFO - 22/07/06 07:53:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:53:31,126] {spark_submit.py:485} INFO - 22/07/06 07:53:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:53:46,126] {spark_submit.py:485} INFO - 22/07/06 07:53:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:54:01,126] {spark_submit.py:485} INFO - 22/07/06 07:54:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:54:16,126] {spark_submit.py:485} INFO - 22/07/06 07:54:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:54:31,126] {spark_submit.py:485} INFO - 22/07/06 07:54:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:54:46,125] {spark_submit.py:485} INFO - 22/07/06 07:54:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:55:01,128] {spark_submit.py:485} INFO - 22/07/06 07:55:01 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:55:16,126] {spark_submit.py:485} INFO - 22/07/06 07:55:16 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:55:31,125] {spark_submit.py:485} INFO - 22/07/06 07:55:31 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:55:46,126] {spark_submit.py:485} INFO - 22/07/06 07:55:46 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:55:53,663] {local_task_job.py:82} ERROR - Received SIGTERM. Terminating subprocesses
[2022-07-06 07:55:55,709] {process_utils.py:124} INFO - Sending Signals.SIGTERM to group 51. PIDs of all processes in the group: [53, 143, 51]
[2022-07-06 07:55:55,722] {process_utils.py:75} INFO - Sending the signal Signals.SIGTERM to group 51
[2022-07-06 07:55:55,865] {taskinstance.py:1408} ERROR - Received SIGTERM. Terminating subprocesses.
[2022-07-06 07:55:56,312] {spark_submit.py:617} INFO - Sending kill signal to spark-submit

[2022-07-06 07:18:58,092] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: spark-test.spark_job manual__2022-07-06T07:18:53.893064+00:00 [queued]>
[2022-07-06 07:18:58,126] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: spark-test.spark_job manual__2022-07-06T07:18:53.893064+00:00 [queued]>
[2022-07-06 07:18:58,129] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2022-07-06 07:18:58,132] {taskinstance.py:1239} INFO - Starting attempt 1 of 2
[2022-07-06 07:18:58,139] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2022-07-06 07:18:58,179] {taskinstance.py:1259} INFO - Executing <Task(SparkSubmitOperator): spark_job> on 2022-07-06 07:18:53.893064+00:00
[2022-07-06 07:18:58,197] {standard_task_runner.py:52} INFO - Started process 49 to run task
[2022-07-06 07:18:58,226] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'spark-test', 'spark_job', 'manual__2022-07-06T07:18:53.893064+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/spark-test.py', '--cfg-path', '/tmp/tmpg76t19_0', '--error-file', '/tmp/tmpr8fhqjee']
[2022-07-06 07:18:58,228] {standard_task_runner.py:77} INFO - Job 2: Subtask spark_job
[2022-07-06 07:18:58,531] {logging_mixin.py:109} INFO - Running <TaskInstance: spark-test.spark_job manual__2022-07-06T07:18:53.893064+00:00 [running]> on host 1f9f6fb14992
[2022-07-06 07:18:59,548] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=***@***.com
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=spark-test
AIRFLOW_CTX_TASK_ID=spark_job
AIRFLOW_CTX_EXECUTION_DATE=2022-07-06T07:18:53.893064+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2022-07-06T07:18:53.893064+00:00
[2022-07-06 07:18:59,631] {base.py:79} INFO - Using connection to: id: spark_connection. Host: spark://spark, Port: 7077, Schema: , Login: , Password: None, extra: {'queue': 'root.default'}
[2022-07-06 07:18:59,636] {spark_submit.py:334} INFO - Spark-Submit cmd: spark-submit --master spark://spark:7077 --conf spark.master=spark://spark:7077 --name Spark Hello World --verbose --queue root.default /usr/local/spark/app/hello-world.py /usr/local/spark/resources/data/***.cfg
[2022-07-06 07:19:07,334] {spark_submit.py:485} INFO - Using properties file: null
[2022-07-06 07:19:07,704] {spark_submit.py:485} INFO - WARNING: An illegal reflective access operation has occurred
[2022-07-06 07:19:07,708] {spark_submit.py:485} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/spark/spark-3.0.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2022-07-06 07:19:07,711] {spark_submit.py:485} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2022-07-06 07:19:07,713] {spark_submit.py:485} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2022-07-06 07:19:07,714] {spark_submit.py:485} INFO - WARNING: All illegal access operations will be denied in a future release
[2022-07-06 07:19:07,856] {spark_submit.py:485} INFO - Parsed arguments:
[2022-07-06 07:19:07,857] {spark_submit.py:485} INFO - master                  spark://spark:7077
[2022-07-06 07:19:07,859] {spark_submit.py:485} INFO - deployMode              null
[2022-07-06 07:19:07,860] {spark_submit.py:485} INFO - executorMemory          null
[2022-07-06 07:19:07,863] {spark_submit.py:485} INFO - executorCores           null
[2022-07-06 07:19:07,865] {spark_submit.py:485} INFO - totalExecutorCores      null
[2022-07-06 07:19:07,867] {spark_submit.py:485} INFO - propertiesFile          null
[2022-07-06 07:19:07,869] {spark_submit.py:485} INFO - driverMemory            null
[2022-07-06 07:19:07,872] {spark_submit.py:485} INFO - driverCores             null
[2022-07-06 07:19:07,874] {spark_submit.py:485} INFO - driverExtraClassPath    null
[2022-07-06 07:19:07,876] {spark_submit.py:485} INFO - driverExtraLibraryPath  null
[2022-07-06 07:19:07,882] {spark_submit.py:485} INFO - driverExtraJavaOptions  null
[2022-07-06 07:19:07,885] {spark_submit.py:485} INFO - supervise               false
[2022-07-06 07:19:07,889] {spark_submit.py:485} INFO - queue                   root.default
[2022-07-06 07:19:07,891] {spark_submit.py:485} INFO - numExecutors            null
[2022-07-06 07:19:07,894] {spark_submit.py:485} INFO - files                   null
[2022-07-06 07:19:07,897] {spark_submit.py:485} INFO - pyFiles                 null
[2022-07-06 07:19:07,904] {spark_submit.py:485} INFO - archives                null
[2022-07-06 07:19:07,907] {spark_submit.py:485} INFO - mainClass               null
[2022-07-06 07:19:07,912] {spark_submit.py:485} INFO - primaryResource         file:/usr/local/spark/app/hello-world.py
[2022-07-06 07:19:07,915] {spark_submit.py:485} INFO - name                    Spark Hello World
[2022-07-06 07:19:07,923] {spark_submit.py:485} INFO - childArgs               [/usr/local/spark/resources/data/***.cfg]
[2022-07-06 07:19:07,925] {spark_submit.py:485} INFO - jars                    null
[2022-07-06 07:19:07,927] {spark_submit.py:485} INFO - packages                null
[2022-07-06 07:19:07,931] {spark_submit.py:485} INFO - packagesExclusions      null
[2022-07-06 07:19:07,933] {spark_submit.py:485} INFO - repositories            null
[2022-07-06 07:19:07,934] {spark_submit.py:485} INFO - verbose                 true
[2022-07-06 07:19:07,938] {spark_submit.py:485} INFO - 
[2022-07-06 07:19:07,940] {spark_submit.py:485} INFO - Spark properties used, including those specified through
[2022-07-06 07:19:07,941] {spark_submit.py:485} INFO - --conf and those from the properties file null:
[2022-07-06 07:19:07,944] {spark_submit.py:485} INFO - (spark.master,spark://spark:7077)
[2022-07-06 07:19:07,946] {spark_submit.py:485} INFO - 
[2022-07-06 07:19:07,947] {spark_submit.py:485} INFO - 
[2022-07-06 07:19:09,584] {spark_submit.py:485} INFO - 22/07/06 07:19:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-07-06 07:19:10,291] {spark_submit.py:485} INFO - Main class:
[2022-07-06 07:19:10,302] {spark_submit.py:485} INFO - org.apache.spark.deploy.PythonRunner
[2022-07-06 07:19:10,304] {spark_submit.py:485} INFO - Arguments:
[2022-07-06 07:19:10,306] {spark_submit.py:485} INFO - file:/usr/local/spark/app/hello-world.py
[2022-07-06 07:19:10,308] {spark_submit.py:485} INFO - null
[2022-07-06 07:19:10,309] {spark_submit.py:485} INFO - /usr/local/spark/resources/data/***.cfg
[2022-07-06 07:19:10,311] {spark_submit.py:485} INFO - Spark config:
[2022-07-06 07:19:10,316] {spark_submit.py:485} INFO - (spark.master,spark://spark:7077)
[2022-07-06 07:19:10,318] {spark_submit.py:485} INFO - (spark.app.name,Spark Hello World)
[2022-07-06 07:19:10,320] {spark_submit.py:485} INFO - (spark.submit.pyFiles,)
[2022-07-06 07:19:10,322] {spark_submit.py:485} INFO - (spark.submit.deployMode,client)
[2022-07-06 07:19:10,324] {spark_submit.py:485} INFO - Classpath elements:
[2022-07-06 07:19:10,326] {spark_submit.py:485} INFO - 
[2022-07-06 07:19:10,328] {spark_submit.py:485} INFO - 
[2022-07-06 07:19:10,329] {spark_submit.py:485} INFO - 
[2022-07-06 07:19:13,295] {spark_submit.py:485} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-07-06 07:19:13,329] {spark_submit.py:485} INFO - 22/07/06 07:19:13 INFO SparkContext: Running Spark version 3.0.1
[2022-07-06 07:19:13,453] {spark_submit.py:485} INFO - 22/07/06 07:19:13 INFO ResourceUtils: ==============================================================
[2022-07-06 07:19:13,457] {spark_submit.py:485} INFO - 22/07/06 07:19:13 INFO ResourceUtils: Resources for spark.driver:
[2022-07-06 07:19:13,459] {spark_submit.py:485} INFO - 
[2022-07-06 07:19:13,462] {spark_submit.py:485} INFO - 22/07/06 07:19:13 INFO ResourceUtils: ==============================================================
[2022-07-06 07:19:13,465] {spark_submit.py:485} INFO - 22/07/06 07:19:13 INFO SparkContext: Submitted application: Spark Hello World
[2022-07-06 07:19:13,674] {spark_submit.py:485} INFO - 22/07/06 07:19:13 INFO SecurityManager: Changing view acls to: default
[2022-07-06 07:19:13,678] {spark_submit.py:485} INFO - 22/07/06 07:19:13 INFO SecurityManager: Changing modify acls to: default
[2022-07-06 07:19:13,680] {spark_submit.py:485} INFO - 22/07/06 07:19:13 INFO SecurityManager: Changing view acls groups to:
[2022-07-06 07:19:13,682] {spark_submit.py:485} INFO - 22/07/06 07:19:13 INFO SecurityManager: Changing modify acls groups to:
[2022-07-06 07:19:13,684] {spark_submit.py:485} INFO - 22/07/06 07:19:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(default); groups with view permissions: Set(); users  with modify permissions: Set(default); groups with modify permissions: Set()
[2022-07-06 07:19:16,521] {spark_submit.py:485} INFO - 22/07/06 07:19:16 INFO Utils: Successfully started service 'sparkDriver' on port 46597.
[2022-07-06 07:19:16,890] {spark_submit.py:485} INFO - 22/07/06 07:19:16 INFO SparkEnv: Registering MapOutputTracker
[2022-07-06 07:19:17,368] {spark_submit.py:485} INFO - 22/07/06 07:19:17 INFO SparkEnv: Registering BlockManagerMaster
[2022-07-06 07:19:17,758] {spark_submit.py:485} INFO - 22/07/06 07:19:17 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-07-06 07:19:17,764] {spark_submit.py:485} INFO - 22/07/06 07:19:17 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-07-06 07:19:17,811] {spark_submit.py:485} INFO - 22/07/06 07:19:17 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-07-06 07:19:17,895] {spark_submit.py:485} INFO - 22/07/06 07:19:17 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b4fa4ac0-c870-4633-b12d-3b904f28bf06
[2022-07-06 07:19:18,009] {spark_submit.py:485} INFO - 22/07/06 07:19:18 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2022-07-06 07:19:18,086] {spark_submit.py:485} INFO - 22/07/06 07:19:18 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-07-06 07:19:19,094] {spark_submit.py:485} INFO - 22/07/06 07:19:19 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2022-07-06 07:19:19,285] {spark_submit.py:485} INFO - 22/07/06 07:19:19 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://1f9f6fb14992:4040
[2022-07-06 07:19:19,519] {spark_submit.py:485} INFO - 22/07/06 07:19:19 WARN SparkContext: Please ensure that the number of slots available on your executors is limited by the number of cores to task cpus and not another custom resource. If cores is not the limiting resource then dynamic allocation will not work properly!
[2022-07-06 07:19:20,899] {spark_submit.py:485} INFO - 22/07/06 07:19:20 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark:7077...
[2022-07-06 07:19:21,399] {spark_submit.py:485} INFO - 22/07/06 07:19:21 INFO TransportClientFactory: Successfully created connection to spark/172.19.0.6:7077 after 376 ms (0 ms spent in bootstraps)
[2022-07-06 07:19:22,818] {spark_submit.py:485} INFO - 22/07/06 07:19:22 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20220706071922-0000
[2022-07-06 07:19:22,869] {spark_submit.py:485} INFO - 22/07/06 07:19:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42861.
[2022-07-06 07:19:22,879] {spark_submit.py:485} INFO - 22/07/06 07:19:22 INFO NettyBlockTransferService: Server created on 1f9f6fb14992:42861
[2022-07-06 07:19:22,923] {spark_submit.py:485} INFO - 22/07/06 07:19:22 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-07-06 07:19:23,211] {spark_submit.py:485} INFO - 22/07/06 07:19:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1f9f6fb14992, 42861, None)
[2022-07-06 07:19:23,263] {spark_submit.py:485} INFO - 22/07/06 07:19:23 INFO BlockManagerMasterEndpoint: Registering block manager 1f9f6fb14992:42861 with 434.4 MiB RAM, BlockManagerId(driver, 1f9f6fb14992, 42861, None)
[2022-07-06 07:19:23,307] {spark_submit.py:485} INFO - 22/07/06 07:19:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 1f9f6fb14992, 42861, None)
[2022-07-06 07:19:23,316] {spark_submit.py:485} INFO - 22/07/06 07:19:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 1f9f6fb14992, 42861, None)
[2022-07-06 07:19:24,540] {spark_submit.py:485} INFO - 22/07/06 07:19:24 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2022-07-06 07:19:26,395] {spark_submit.py:485} INFO - 22/07/06 07:19:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 175.8 KiB, free 434.2 MiB)
[2022-07-06 07:19:26,636] {spark_submit.py:485} INFO - 22/07/06 07:19:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.1 KiB, free 434.2 MiB)
[2022-07-06 07:19:26,651] {spark_submit.py:485} INFO - 22/07/06 07:19:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 1f9f6fb14992:42861 (size: 27.1 KiB, free: 434.4 MiB)
[2022-07-06 07:19:26,669] {spark_submit.py:485} INFO - 22/07/06 07:19:26 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0
[2022-07-06 07:19:27,255] {spark_submit.py:485} INFO - 22/07/06 07:19:27 INFO FileInputFormat: Total input files to process : 1
[2022-07-06 07:19:27,353] {spark_submit.py:485} INFO - 22/07/06 07:19:27 INFO SparkContext: Starting job: count at /usr/local/spark/app/hello-world.py:23
[2022-07-06 07:19:27,392] {spark_submit.py:485} INFO - 22/07/06 07:19:27 INFO DAGScheduler: Got job 0 (count at /usr/local/spark/app/hello-world.py:23) with 2 output partitions
[2022-07-06 07:19:27,397] {spark_submit.py:485} INFO - 22/07/06 07:19:27 INFO DAGScheduler: Final stage: ResultStage 0 (count at /usr/local/spark/app/hello-world.py:23)
[2022-07-06 07:19:27,404] {spark_submit.py:485} INFO - 22/07/06 07:19:27 INFO DAGScheduler: Parents of final stage: List()
[2022-07-06 07:19:27,428] {spark_submit.py:485} INFO - 22/07/06 07:19:27 INFO DAGScheduler: Missing parents: List()
[2022-07-06 07:19:27,452] {spark_submit.py:485} INFO - 22/07/06 07:19:27 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /usr/local/spark/app/hello-world.py:23), which has no missing parents
[2022-07-06 07:19:27,563] {spark_submit.py:485} INFO - 22/07/06 07:19:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.2 KiB, free 434.2 MiB)
[2022-07-06 07:19:27,589] {spark_submit.py:485} INFO - 22/07/06 07:19:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 434.2 MiB)
[2022-07-06 07:19:27,604] {spark_submit.py:485} INFO - 22/07/06 07:19:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 1f9f6fb14992:42861 (size: 4.8 KiB, free: 434.4 MiB)
[2022-07-06 07:19:27,613] {spark_submit.py:485} INFO - 22/07/06 07:19:27 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
[2022-07-06 07:19:27,682] {spark_submit.py:485} INFO - 22/07/06 07:19:27 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at count at /usr/local/spark/app/hello-world.py:23) (first 15 tasks are for partitions Vector(0, 1))
[2022-07-06 07:19:27,693] {spark_submit.py:485} INFO - 22/07/06 07:19:27 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
[2022-07-06 07:19:42,817] {spark_submit.py:485} INFO - 22/07/06 07:19:42 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:19:57,814] {spark_submit.py:485} INFO - 22/07/06 07:19:57 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:20:12,816] {spark_submit.py:485} INFO - 22/07/06 07:20:12 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:20:27,816] {spark_submit.py:485} INFO - 22/07/06 07:20:27 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:20:42,814] {spark_submit.py:485} INFO - 22/07/06 07:20:42 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:20:57,813] {spark_submit.py:485} INFO - 22/07/06 07:20:57 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:21:12,813] {spark_submit.py:485} INFO - 22/07/06 07:21:12 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:21:27,816] {spark_submit.py:485} INFO - 22/07/06 07:21:27 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:21:42,821] {spark_submit.py:485} INFO - 22/07/06 07:21:42 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:21:57,813] {spark_submit.py:485} INFO - 22/07/06 07:21:57 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:22:12,820] {spark_submit.py:485} INFO - 22/07/06 07:22:12 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:22:27,813] {spark_submit.py:485} INFO - 22/07/06 07:22:27 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:22:42,814] {spark_submit.py:485} INFO - 22/07/06 07:22:42 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-07-06 07:22:49,425] {local_task_job.py:82} ERROR - Received SIGTERM. Terminating subprocesses
[2022-07-06 07:22:50,347] {process_utils.py:124} INFO - Sending Signals.SIGTERM to group 49. PIDs of all processes in the group: [50, 94, 49]
[2022-07-06 07:22:50,349] {process_utils.py:75} INFO - Sending the signal Signals.SIGTERM to group 49
[2022-07-06 07:22:50,571] {taskinstance.py:1408} ERROR - Received SIGTERM. Terminating subprocesses.
[2022-07-06 07:22:50,595] {spark_submit.py:617} INFO - Sending kill signal to spark-submit
[2022-07-06 07:22:59,053] {taskinstance.py:1700} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 404, in submit
    self._process_spark_submit_log(iter(self._submit_sp.stdout))  # type: ignore
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 453, in _process_spark_submit_log
    for line in itr:
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1410, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal
